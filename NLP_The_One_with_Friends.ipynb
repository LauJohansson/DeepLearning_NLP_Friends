{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lau kopi af Merge af NLP Friends_inkl_tSNE_and_smartpath.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "PhzDijSPm9no"
      ],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LauJohansson/DeepLearning_NLP_Friends/blob/master/NLP_The_One_with_Friends.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YiIMKIBRmeI_",
        "colab_type": "text"
      },
      "source": [
        "#**Introduction**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twGMoGa7mhCd",
        "colab_type": "text"
      },
      "source": [
        "We build a Recurrent Neural Network (RNN) using the Long Short-Term Memory (LSTM) architecture. Through tuning of hyperparameters and regularization through variational sequence length and DropConnect we achieve a perplexity value on the Penn Treebank dataset. We then utilize the found model to generate dialogue for the TV-series “Friends”. To this end, we pretrain the model on Wikipedia text and further tune the hyperparameters to fit “Friends”. Finally, we show a subset of the predicted dialogue and visualise the found semantics of “Friends” through t-SNE on the embedding weights.  \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LtIKDkJSg2XV",
        "colab_type": "text"
      },
      "source": [
        "NB: When opening from Github you can not load or save pretrained model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhzDijSPm9no",
        "colab_type": "text"
      },
      "source": [
        "##Acknowledgements"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wX9wD9wbnJSY",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "This notebook is inspired by the following NLP Tutorial:https://github.com/graykode/nlp-tutorial\n",
        "Made by Tae-Hwan Jung. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DK8RFoquSj5",
        "colab_type": "text"
      },
      "source": [
        "# **Initialising Friends data**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLpYqib8PeGH",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://drive.google.com/uc?id=1JzPdBavHFL-8R1Tc0F6z9KlELR326MXy)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64wMafbF9Xxl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "cf0f3711-c2b1-4f0c-f76a-42186c1ed1dd"
      },
      "source": [
        "#Imports\n",
        "import collections\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import glob\n",
        "import io\n",
        "import matplotlib. pyplot as plt\n",
        "from collections import Counter\n",
        "# Importing drive method from colab for accessing google drive\n",
        "from google.colab import drive\n",
        "\n",
        "# Mounting drive\n",
        "## This will require authentication : Follow the steps as guided\n",
        "\n",
        "##Choose your path for saving and loading the trained net:\n",
        "\n",
        "##Default path:\n",
        "#PATH=\"/content/drive/My Drive/\"\n",
        "\n",
        "#Authors path:\n",
        "PATH=\"/content/drive/My Drive/DL_project/\"\n",
        "\n",
        "drive.mount('/content/drive',force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84ed6CSdPlRY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Loading data from Github\n",
        "## If your are one of the authors or you have the files located on your google drive - google drive is used. \n",
        "\n",
        "try: #If files in google drive\n",
        "  datafrom='Drive'\n",
        "  friends_train = open(PATH+\"friends_train.txt\").read()\n",
        "  friends_valid = open(PATH+\"friends_valid.txt\").read()\n",
        "  friends_test = open(PATH+\"friends_test.txt\").read()\n",
        "\n",
        "  pretraining_train = open(PATH+\"wikitrainclean.txt\").read()\n",
        "  pretraining_valid = open(PATH+\"wikivalidclean.txt\").read()\n",
        "  pretraining_test = open(PATH+\"wikitestclean.txt\").read()\n",
        "  \n",
        "except: #For github version\n",
        "  datafrom='Github'\n",
        "  from urllib.request import urlopen\n",
        "\n",
        "  friends_train=str(urlopen('https://raw.githubusercontent.com/LauJohansson/DeepLearning_NLP_Friends/master/Data/friends_train.txt').read(),encoding=\"utf-8\")\n",
        "  friends_valid = str(urlopen('https://raw.githubusercontent.com/LauJohansson/DeepLearning_NLP_Friends/master/Data/friends_valid.txt').read(),encoding=\"utf-8\")\n",
        "  friends_test = str(urlopen('https://raw.githubusercontent.com/LauJohansson/DeepLearning_NLP_Friends/master/Data/friends_test.txt').read(),encoding=\"utf-8\")\n",
        "\n",
        "  pretraining_train = str(urlopen('https://raw.githubusercontent.com/LauJohansson/DeepLearning_NLP_Friends/master/Data/seinfeldtrainclean.txt').read(),encoding=\"utf-8\")\n",
        "  pretraining_valid = str(urlopen('https://raw.githubusercontent.com/LauJohansson/DeepLearning_NLP_Friends/master/Data/seinfeldvalidclean.txt').read(),encoding=\"utf-8\")\n",
        "  pretraining_test = str(urlopen('https://raw.githubusercontent.com/LauJohansson/DeepLearning_NLP_Friends/master/Data/seinfeldtestclean.txt').read(),encoding=\"utf-8\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vq9vje2nvas2",
        "colab_type": "code",
        "outputId": "84bacaa4-4902-493b-da87-5c30d2358b47",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Check where you get data from:\n",
        "datafrom"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Drive'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIQl-Df3TiEZ",
        "colab_type": "code",
        "outputId": "c42d7ebe-91e7-41da-8dd0-2811cdd03efd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "# check data\n",
        "print(\"Traindata snippet friends:    \" + friends_train[:100])\n",
        "print(\"Validdata snippet friends:    \" + friends_valid[:100])\n",
        "print(\"Testdata snippet friends:     \" + friends_test[:100])\n",
        "print()\n",
        "print(\"Traindata snippet pretraining:       \" + pretraining_train[:100])\n",
        "print(\"Validdata snippet pretraining:       \" + pretraining_valid[:100])\n",
        "print(\"Testdata snippet pretraining:        \" + pretraining_test[:100])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Traindata snippet friends:    [ scene: central perk , chandler , joey , phoebe , and monica are there . ] monica: there's nothing \n",
            "Validdata snippet friends:    parts that i really wanted . you always believed in me man . even , even when i didn't believe in my\n",
            "Testdata snippet friends:     is at the half - opened door ] phoebe: ( in a strange heavy accent ) hello <unk> , it's time for you\n",
            "\n",
            "Traindata snippet pretraining:       valkyria chronicles iii senj\\u014d no valkyria 3 : <unk> chronicles ( japanese : \\u6226\\u5834\\u306e\\\n",
            "Validdata snippet pretraining:       homarus gammarus homarus gammarus , known as the european lobster or common lobster , is a species o\n",
            "Testdata snippet pretraining:        robert <unk> robert <unk> is an english film , television and theatre actor . he had a guest @ - @ s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvq6sH9HSqtx",
        "colab_type": "code",
        "outputId": "aee81931-2006-4eb0-b8f3-f773c75e6e1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Find words that occur in friends data, but not in pretraining data\n",
        "words_notinpretraining = list(set(friends_train.split()).difference(pretraining_train.split()))\n",
        "words_notinpretraining[0:5]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ding', 'craps', 'back...', 'porsche', 'counselor']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBJq1zyuuGkz",
        "colab_type": "text"
      },
      "source": [
        "# **Initialising Penn Tree Bank data**\n",
        "![alt text](https://drive.google.com/uc?id=1A8SY9dmsLx2hoanWWWhT_IJnJcJOmluR)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAp6BEbqQEDe",
        "colab_type": "text"
      },
      "source": [
        "The following site is used as inspiration to get PTB data: https://corochann.com/penn-tree-bank-ptb-dataset-introduction-1456.html\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zoW2qb_YQMil",
        "colab_type": "code",
        "outputId": "4b12a5bd-67e0-46bc-e284-d4f598e9f208",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "'''\n",
        "#Get PTB data\n",
        "from __future__ import print_function\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        " \n",
        " \n",
        "import numpy as np\n",
        " \n",
        "import chainer\n",
        "train, val, test = chainer.datasets.get_ptb_words()\n",
        "\n",
        "ptb_dict = chainer.datasets.get_ptb_words_vocabulary()\n",
        "print('Number of vocabulary', len(ptb_dict))\n",
        "print('ptb_dict', ptb_dict)\n",
        "\n",
        "ptb_word_id_dict = ptb_dict\n",
        "ptb_id_word_dict = dict((v,k) for k,v in ptb_word_id_dict.items())\n",
        "\n",
        "\n",
        "#Create data as str\n",
        "dataPTB=' '.join([ptb_id_word_dict[i] for i in train[:]])\n",
        "dataPTBtest=' '.join([ptb_id_word_dict[i] for i in test[:]])\n",
        "dataPTBvalid=' '.join([ptb_id_word_dict[i] for i in val[:]])\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n#Get PTB data\\nfrom __future__ import print_function\\nimport os\\nimport matplotlib.pyplot as plt\\n%matplotlib inline\\n \\n \\nimport numpy as np\\n \\nimport chainer\\ntrain, val, test = chainer.datasets.get_ptb_words()\\n\\nptb_dict = chainer.datasets.get_ptb_words_vocabulary()\\nprint('Number of vocabulary', len(ptb_dict))\\nprint('ptb_dict', ptb_dict)\\n\\nptb_word_id_dict = ptb_dict\\nptb_id_word_dict = dict((v,k) for k,v in ptb_word_id_dict.items())\\n\\n\\n#Create data as str\\ndataPTB=' '.join([ptb_id_word_dict[i] for i in train[:]])\\ndataPTBtest=' '.join([ptb_id_word_dict[i] for i in test[:]])\\ndataPTBvalid=' '.join([ptb_id_word_dict[i] for i in val[:]])\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIVP8LXaNyGA",
        "colab_type": "text"
      },
      "source": [
        "#***Import and initializing hyperparameters***\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2otEIPN5TPc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import os\n",
        "from argparse import Namespace\n",
        "\n",
        "flags = Namespace(\n",
        "    mode = 'friends', #'pretraining' is pretraining and 'friends' is fine-tuning.\n",
        "    name='Friends',\n",
        "    train_file_pretraining = pretraining_train,                    #shift between data (data or dataPTB)\n",
        "    valid_file_pretraining = pretraining_valid, \n",
        "    test_file_pretraining = pretraining_test,   \n",
        "    train_file_friends= friends_train,                    #shift between data (data or dataPTB)\n",
        "    valid_file_friends= friends_valid, \n",
        "    test_file_friends = friends_test,\n",
        "    seq_size= 32, #16,\n",
        "    batch_size=40, #16,\n",
        "    embedding_size=256, #300,\n",
        "    lstm_size=512, #,300,                      #Hidden size\n",
        "    gradients_norm=0.5,\n",
        "    #initial_words=['banknote', 'berlitz'],\n",
        "    initial_words_train = [],\n",
        "    initial_words_valid= [],\n",
        "    predict_top_k=5,                    #Choose the k best next_word_prediction, and a random is chosing.\n",
        "    checkpoint_path='checkpoint',\n",
        "    total_epochs=80,   #30                  #Change number of epochs in training\n",
        "    learning_rate=0.001,\n",
        "    predict_every=1000,\n",
        "    #validation_corpus_size=len(valid_file.split()),\n",
        "    dropconnect_rate=0.4,\n",
        "    n_lay=2,\n",
        "\n",
        "    #Set variational sequence length on/off\n",
        "    var_seq='Y',                            #Choose if variational sequence length is on/off\n",
        "    var_seq_std=2,                          #Choose std. dev. for norm distribution for var. seq. length ( in moment 1/2 of seq length)\n",
        "\n",
        "\n",
        "    #scheduler parameters\n",
        "    schedule_on='N',                        #Choose if LR-scheduler is on/off\n",
        "    triangular='N',                         #Choose 'Y' to turn on the slanted triangular LR. \n",
        "    cut_fracI=0.2,                          #Choose the fraction of iterations we increase the LR in STL\n",
        "    ratioI=32,                              #Choose how much smaller the lowest LR is from the maximum LR ηmax\n",
        "    nmaxI=0.0,                              #this will be set = learning_rate \n",
        "\n",
        "    #Use same drop-mask for drop-connect\n",
        "    same_drop_lstm='N',                     #Choose 'Y' if drop-connect all should use same mask\n",
        "    \n",
        "    #Dropout on embedding layer\n",
        "    drop_embed=0.5,                         #Choose dropoutrate for embedding dropout        \n",
        "\n",
        "    #Optimizer selection\n",
        "    optim_select='AdamW'                      #Choose between \"AdamW, SGD, ASGD\"\n",
        "\n",
        ")\n",
        "\n",
        "\n",
        "#initialising two random words, the train and test data should use\n",
        "if flags.mode == 'friends':\n",
        "  for i in range(0,1):\n",
        "    flags.initial_words_train.append(np.random.choice(flags.train_file_friends.split()))\n",
        "    flags.initial_words_valid.append(np.random.choice(flags.valid_file_friends.split()))\n",
        "    flags.validation_corpus_size = len(flags.valid_file_friends.split())\n",
        "    \n",
        "if flags.mode == 'pretraining':\n",
        "  for i in range(0,1):\n",
        "    flags.initial_words_train.append(np.random.choice(flags.train_file_pretraining.split()))\n",
        "    flags.initial_words_valid.append(np.random.choice(flags.valid_file_pretraining.split()))\n",
        "    flags.validation_corpus_size = len(flags.valid_file_pretraining.split())\n",
        "\n",
        "flags.nmaxI=flags.learning_rate"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKozVznDlxMS",
        "colab_type": "text"
      },
      "source": [
        "#**Define functions and set up model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYSUF18ZN6cf",
        "colab_type": "text"
      },
      "source": [
        "##Function to get data for main and batch-function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjlDy7cq5UA8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_data_from_file(train_file, batch_size, seq_size):\n",
        "  \n",
        "    text=train_file.split()\n",
        "    # Extend words_notinpretraining to text to get them as a part of the mapping dictionary\n",
        "    text.extend(words_notinpretraining)\n",
        "    word_counts = Counter(text)\n",
        "    sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
        "    int_to_vocab = {k: w for k, w in enumerate(sorted_vocab)}\n",
        "    vocab_to_int = {w: k for k, w in int_to_vocab.items()}\n",
        "    n_vocab = len(int_to_vocab)\n",
        "\n",
        "    text=train_file.split()\n",
        "\n",
        "    print('Vocabulary size', n_vocab)\n",
        "\n",
        "    int_text = [vocab_to_int[w] for w in text]\n",
        "    num_batches = int(len(int_text) / (seq_size * batch_size))\n",
        "    in_text = int_text[:num_batches * batch_size * seq_size]\n",
        "    out_text = np.zeros_like(in_text)\n",
        "    out_text[:-1] = in_text[1:]\n",
        "    out_text[-1] = in_text[0]\n",
        "    in_text = np.reshape(in_text, (batch_size, -1))\n",
        "    out_text = np.reshape(out_text, (batch_size, -1))\n",
        "    return int_to_vocab, vocab_to_int, n_vocab, in_text, out_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7pPsjVmq5Wes",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_batches(in_text, out_text, batch_size, seq_size):\n",
        "    num_batches = np.prod(in_text.shape) // (seq_size * batch_size)\n",
        "    for i in range(0, num_batches * seq_size, seq_size):\n",
        "        yield in_text[:, i:i+seq_size], out_text[:, i:i+seq_size]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VkuuPkykOH6k",
        "colab_type": "text"
      },
      "source": [
        "##Setting up the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncUCPVoc5bLd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RNNModule(nn.Module):\n",
        "    def __init__(self, n_vocab, seq_size, embedding_size, lstm_size):\n",
        "        super(RNNModule, self).__init__()\n",
        "        self.seq_size = seq_size\n",
        "        self.lstm_size = lstm_size\n",
        "        self.embedding = nn.Embedding(n_vocab, embedding_size)\n",
        "        self.lstm=nn.LSTM(embedding_size,\n",
        "                            lstm_size,\n",
        "                            batch_first=True,num_layers=flags.n_lay)\n",
        "        \n",
        "        self.dense = nn.Linear(lstm_size, n_vocab)\n",
        "        self.drop_out=nn.Dropout(flags.drop_embed)\n",
        "        self.OneMaskOnly=torch.autograd.Variable(torch.Tensor(getattr(self.lstm,self.lstm._all_weights[0][0]).shape[0],\n",
        "                                                        getattr(self.lstm,self.lstm._all_weights[0][0]).shape[1]).uniform_().to(\"cuda\") > flags.dropconnect_rate)\n",
        "\n",
        "\n",
        "    def forward(self, x, prev_state):\n",
        "        embed = self.embedding(x)\n",
        "        embed=self.drop_out(embed)\n",
        "        orig=[]\n",
        "\n",
        "        #Make dropconnect\n",
        "        if self.training:\n",
        "          for i in range( len(self.lstm._all_weights[0])):\n",
        "            name = self.lstm._all_weights[0][i]\n",
        "            if name.find('LSTM.weight_hh_l')!=-1:      \n",
        "              orig.append(getattr(self.lstm,name))\n",
        "            \n",
        "              if flags.same_drop_lstm=='Y':\n",
        "                mask=self.OneMaskOnly\n",
        "              else:\n",
        "                mask=torch.autograd.Variable(torch.Tensor(getattr(self.lstm,name).shape[0],\n",
        "                                                        getattr(self.lstm,name).shape[1]).uniform_().to(\"cuda\") > flags.flags.dropconnect_rate)\n",
        "              setattr(self.lstm,name,torch.nn.Parameter(torch.mul(getattr(self.lstm,name),mask)))\n",
        "              \n",
        "              self.lstm.flatten_parameters()\n",
        "             \n",
        "\n",
        "        #LSTM forward\n",
        "        output, state = self.lstm(embed, prev_state)\n",
        "\n",
        "        #Set hh-weight back to original\n",
        "        if self.training:\n",
        "          a=0\n",
        "          for i in range( len(self.lstm._all_weights[0])):\n",
        "            name = self.lstm._all_weights[0][i]\n",
        "            if name.find('LSTM.weight_hh_l')!=-1:\n",
        "              print(orig)\n",
        "              setattr(self.lstm,name,orig[a])\n",
        "              #self.lstm.weight_hh_l0=orig\n",
        "              self.lstm.flatten_parameters()\n",
        "              a=+1\n",
        "\n",
        "        logits = self.dense(output)\n",
        "\n",
        "        return logits, state\n",
        "       \n",
        "    def zero_state(self, batch_size):\n",
        "        return (torch.zeros(flags.n_lay, batch_size, self.lstm_size),\n",
        "                torch.zeros(flags.n_lay, batch_size, self.lstm_size))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kf20oH-l9pIZ",
        "colab_type": "text"
      },
      "source": [
        "##Defining ASGD\n",
        "from: https://pytorch.org/docs/stable/_modules/torch/optim/asgd.html\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63Qg4X449uXi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import torch\n",
        "#from .optimizer import Optimizer\n",
        "\n",
        "\n",
        "class ASGD(torch.optim.Optimizer):\n",
        "    \"\"\"Implements Averaged Stochastic Gradient Descent.\n",
        "\n",
        "    It has been proposed in `Acceleration of stochastic approximation by\n",
        "    averaging`_.\n",
        "\n",
        "    Arguments:\n",
        "        params (iterable): iterable of parameters to optimize or dicts defining\n",
        "            parameter groups\n",
        "        lr (float, optional): learning rate (default: 1e-2)\n",
        "        lambd (float, optional): decay term (default: 1e-4)\n",
        "        alpha (float, optional): power for eta update (default: 0.75)\n",
        "        t0 (float, optional): point at which to start averaging (default: 1e6)\n",
        "        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n",
        "\n",
        "    .. _Acceleration of stochastic approximation by averaging:\n",
        "        http://dl.acm.org/citation.cfm?id=131098\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, params, lr=1e-2, lambd=1e-4, alpha=0.75, t0=1e6, weight_decay=0):\n",
        "        if not 0.0 <= lr:\n",
        "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
        "        if not 0.0 <= weight_decay:\n",
        "            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n",
        "\n",
        "        defaults = dict(lr=lr, lambd=lambd, alpha=alpha, t0=t0,\n",
        "                        weight_decay=weight_decay)\n",
        "        super(ASGD, self).__init__(params, defaults)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        \"\"\"Performs a single optimization step.\n",
        "\n",
        "        Arguments:\n",
        "            closure (callable, optional): A closure that reevaluates the model\n",
        "                and returns the loss.\n",
        "        \"\"\"\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                grad = p.grad.data\n",
        "                if grad.is_sparse:\n",
        "                    raise RuntimeError('ASGD does not support sparse gradients')\n",
        "                state = self.state[p]\n",
        "\n",
        "                # State initialization\n",
        "                if len(state) == 0:\n",
        "                    state['step'] = 0\n",
        "                    state['eta'] = group['lr']\n",
        "                    state['mu'] = 1\n",
        "                    state['ax'] = torch.zeros_like(p.data)\n",
        "\n",
        "                state['step'] += 1\n",
        "\n",
        "                if group['weight_decay'] != 0:\n",
        "                    grad = grad.add(group['weight_decay'], p.data)\n",
        "\n",
        "                # decay term\n",
        "                p.data.mul_(1 - group['lambd'] * state['eta'])\n",
        "\n",
        "                # update parameter\n",
        "                p.data.add_(-state['eta'], grad)\n",
        "\n",
        "                # averaging\n",
        "                if state['mu'] != 1:\n",
        "                    state['ax'].add_(p.data.sub(state['ax']).mul(state['mu']))\n",
        "                else:\n",
        "                    state['ax'].copy_(p.data)\n",
        "\n",
        "                # update eta and mu\n",
        "                state['eta'] = (group['lr'] /\n",
        "                                math.pow((1 + group['lambd'] * group['lr'] * state['step']), group['alpha']))\n",
        "                state['mu'] = 1 / max(1, state['step'] - group['t0'])\n",
        "\n",
        "        return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gb22lHosOTjd",
        "colab_type": "text"
      },
      "source": [
        "##Define function to call optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZl2BaWK5mRx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_loss_and_train_op(net, lrI):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    if (flags.optim_select=='ASGD'):\n",
        "      optimizer = torch.optim.ASGD(net.parameters(), lr=lrI,weight_decay=0,t0=0,lambd=0) #decay, t0 and lambd taken from article\n",
        "    elif (flags.optim_select=='SGD'):\n",
        "      optimizer = torch.optim.SGD(net.parameters(), lr=lrI) #decay, t0 and lambd taken from article\n",
        "    else:  \n",
        "      optimizer = torch.optim.AdamW(net.parameters(), lr=lrI,weight_decay=0.3)\n",
        "  \n",
        "    return criterion, optimizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uy8anIthSTXZ",
        "colab_type": "text"
      },
      "source": [
        "##Define function to schedule optimizer for ASGD\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Lqz2wRTUsAw",
        "colab_type": "text"
      },
      "source": [
        "Article used: [Universal Language Model Fine-tuning for Text Classification](https://www.aclweb.org/anthology/P18-1031.pdf?fbclid=IwAR0-TADs3LWh74b4xbA2QW5OYM5-_5iFu2EBjd_0-KVWOUytnBV5TeS9KGo)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dC3LvPuXSTqm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#T=number of epochs * number of updates per epoch\n",
        "#\"cut_frac\" is the fraction of iterations we increase the LR\n",
        "# \"cut\" is the iteration when we switch from increasing to decreasing the LR\n",
        "# \"p\" is the fraction of the number of iterations we have increased or will decrease the LR respectively\n",
        "# \"ratio\" specifies how much smaller the lowest LR is from the maximum LR nmax\n",
        "#\"nt\"/return value is the learning rate at iteration t.\n",
        "\n",
        "#The paper generally use cut_frac = 0.1, ratio = 32 and ηmax = 0.01\n",
        "\n",
        "\n",
        "def triangular_lr_func(t,T,cut_frac=flags.cut_fracI, ratio=flags.ratioI, nmax=flags.nmaxI ):\n",
        "    cut=np.floor(T*cut_frac)\n",
        "\n",
        "    if (t<cut): p=t/cut\n",
        "    else: p=1-((t-cut)/(cut*(ratio-1)))\n",
        "\n",
        "\n",
        "    return nmax * (1+p*(ratio-1))/ratio"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-OSrXMTeA6i",
        "colab_type": "text"
      },
      "source": [
        "##Custommade scheduler\n",
        "This scheduler is an example on how ASGD LR can be decreased for each iteration. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9aq4WV5lLes_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def easy_schedule(t,nmax=flags.nmaxI):\n",
        "  result = nmax-(t*0.0001)\n",
        "\n",
        "  if(result<0.1): result=0.1\n",
        "\n",
        "  return result\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXgqNYjZOW9X",
        "colab_type": "text"
      },
      "source": [
        "#**Define main**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNF3L6ss5qHL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main():  \n",
        "    torch.manual_seed(11)\n",
        "    print('------------------------------------------------------------------------  ')\n",
        "    print('----------------------Go into training mode--------------------------------  ')\n",
        "    print(\"----------------------using %s data------------------------\" % flags.mode)\n",
        "    print('------------------------------------------------------------------------  ')\n",
        "    \n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    # Pre-training mode on pretrainingtext\n",
        "    # Start network from scratch\n",
        "    if flags.mode == 'pretraining': \n",
        "      int_to_vocab, vocab_to_int, n_vocab, in_text, out_text = get_data_from_file(\n",
        "          flags.train_file_pretraining, flags.batch_size, flags.seq_size)\n",
        "           \n",
        "    # Fine-tuning mode on friends\n",
        "    if flags.mode == 'friends': \n",
        "\n",
        "      # Get int_to_vocab, vocab_to_int, n_vocab from pretraining text pre-training\n",
        "      int_to_vocab, vocab_to_int, n_vocab, _, _ = get_data_from_file( \n",
        "          flags.train_file_pretraining, flags.batch_size, flags.seq_size)\n",
        "      \n",
        "\n",
        "      # Load and store data for friends-training\n",
        "      text = flags.train_file_friends.split()\n",
        "      int_text = [vocab_to_int[v] for v in text]\n",
        "      var1 = len(int_text) % flags.batch_size \n",
        "      in_text =   int_text[:] if var1==0 else  int_text[:-var1]              ## Added -19 to make divisble with 20 batch size\n",
        "      out_text = np.zeros_like(in_text)\n",
        "      out_text[:-1] = in_text[1:]\n",
        "      out_text[-1] = in_text[0]\n",
        "      in_text = np.reshape(in_text, (flags.batch_size, -1))\n",
        "      out_text = np.reshape(out_text, (flags.batch_size, -1))\n",
        "\n",
        "\n",
        "    if flags.mode == 'pretraining': \n",
        "      # Load and store data for validation\n",
        "      valid_text = flags.valid_file_pretraining.split()\n",
        "      valid_int_text = [vocab_to_int[v] for v in valid_text]\n",
        "      var2 = len(valid_int_text) % flags.batch_size\n",
        "      valid_in_text =  valid_int_text[:] if var2==0 else valid_int_text[:-var2] ## Added -19 to make divisble with 20 batch size\n",
        "      valid_out_text = np.zeros_like(valid_in_text)\n",
        "      valid_out_text[:-1] = valid_in_text[1:]\n",
        "      valid_out_text[-1] = valid_in_text[0]\n",
        "      valid_in_text = np.reshape(valid_in_text, (flags.batch_size, -1))\n",
        "      valid_out_text = np.reshape(valid_out_text, (flags.batch_size, -1))\n",
        "\n",
        "      test_text = flags.test_file_pretraining.split()\n",
        "      test_int_text = [vocab_to_int[v] for v in test_text]\n",
        "      var3 = len(test_int_text) % flags.batch_size\n",
        "      test_in_text =  test_int_text[:] if var3==0 else test_int_text[:-var3]\n",
        "      test_out_text = np.zeros_like(test_in_text)\n",
        "      test_out_text[:-1] = test_in_text[1:]\n",
        "      test_out_text[-1] = test_in_text[0]\n",
        "      test_in_text = np.reshape(test_in_text, (flags.batch_size, -1))\n",
        "      test_out_text = np.reshape(test_out_text, (flags.batch_size, -1))\n",
        "\n",
        "    \n",
        "    if flags.mode == 'friends': \n",
        "      # Load and store data for validation\n",
        "      valid_text = flags.valid_file_friends.split()\n",
        "      valid_int_text = [vocab_to_int[v] for v in valid_text]\n",
        "      var4 = len(valid_int_text) % flags.batch_size\n",
        "      valid_in_text = valid_int_text[:] if  var4==0 else  valid_int_text[:-var4]\n",
        "      valid_out_text = np.zeros_like(valid_in_text)\n",
        "      valid_out_text[:-1] = valid_in_text[1:]\n",
        "      valid_out_text[-1] = valid_in_text[0]\n",
        "      valid_in_text = np.reshape(valid_in_text, (flags.batch_size, -1))\n",
        "      valid_out_text = np.reshape(valid_out_text, (flags.batch_size, -1))\n",
        "\n",
        "      test_text = flags.test_file_friends.split()\n",
        "      test_int_text = [vocab_to_int[v] for v in test_text]\n",
        "      var5 = len(test_int_text) % flags.batch_size\n",
        "      test_in_text = test_int_text[:] if  var5==0 else  test_int_text[:-var5]\n",
        "      test_out_text = np.zeros_like(test_in_text)\n",
        "      test_out_text[:-1] = test_in_text[1:]\n",
        "      test_out_text[-1] = test_in_text[0]\n",
        "      test_in_text = np.reshape(test_in_text, (flags.batch_size, -1))\n",
        "      test_out_text = np.reshape(test_out_text, (flags.batch_size, -1))\n",
        "\n",
        "\n",
        "    if flags.mode == 'pretraining': \n",
        "      net = RNNModule(n_vocab, flags.seq_size,\n",
        "                      flags.embedding_size, flags.lstm_size)\n",
        "      net = net.to(device)\n",
        "\n",
        "    if flags.mode == 'friends':  \n",
        "      net = RNNModule(n_vocab, flags.seq_size,\n",
        "                      flags.embedding_size, flags.lstm_size)\n",
        "      if datafrom=='Drive':\n",
        "        net.load_state_dict(torch.load('/content/drive/My Drive/net.pth'))\n",
        "      else:\n",
        "        net.load_state_dict(torch.load('/content/drive/My Drive/net.pth'))\n",
        "      \n",
        "      net = net.to(device)\n",
        "      net.eval()\n",
        "\n",
        "    criterion, optimizer = get_loss_and_train_op(net, flags.learning_rate)\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "    #Documentation from: https://pytorch.org/docs/master/optim.html\n",
        "\n",
        "    if (flags.schedule_on=='Y'):\n",
        "      expected_number_of_epochs= (np.prod(in_text.shape) // (flags.seq_size * flags.batch_size))*flags.total_epochs\n",
        "      \n",
        "    if (flags.triangular=='Y'):\n",
        "        lambda1= lambda iteration_schedule: triangular_lr_func(iteration_schedule,expected_number_of_epochs)\n",
        "    else:\n",
        "        lambda1= lambda iteration_schedule: easy_schedule(iteration_schedule)\n",
        "\n",
        "\n",
        "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda1)\n",
        "\n",
        "    iteration = 0\n",
        "\n",
        "    if (flags.schedule_on=='Y'):\n",
        "      iteration_schedule=0\n",
        "\n",
        "    epochnumber = []\n",
        "    all_train_losses = []\n",
        "    all_valid_losses = []\n",
        "    all_test_losses = []\n",
        "    for e in range(flags.total_epochs): #Implemented flags.total_epochs to change parameter\n",
        "        epochnumber.extend([e+1])\n",
        "\n",
        "        ######################## TRAINING EPOCH ###########################\n",
        "\n",
        "\n",
        "        ###Variational sequence length###\n",
        "\n",
        "        if (flags.var_seq=='Y'):\n",
        "          base_length=np.round(np.random.normal(flags.seq_size, flags.seq_size/flags.var_seq_std)).astype(int)\n",
        "          if (base_length<2):\n",
        "            base_length=2\n",
        "        else:\n",
        "          base_length=flags.seq_size\n",
        "\n",
        "        #########################\n",
        "\n",
        "\n",
        "\n",
        "        batches = get_batches(in_text, out_text, flags.batch_size, base_length)\n",
        "        valid_batches = get_batches(valid_in_text, valid_out_text, flags.batch_size, base_length)\n",
        "        test_batches = get_batches(test_in_text, test_out_text, flags.batch_size, base_length)\n",
        "\n",
        "        state_h, state_c = net.zero_state(flags.batch_size)\n",
        "        \n",
        "        # Transfer data to GPU\n",
        "        state_h = state_h.to(device)\n",
        "        state_c = state_c.to(device)\n",
        "\n",
        "        for x, y in batches:\n",
        "            iteration += 1\n",
        "\n",
        "\n",
        "            if (flags.schedule_on=='Y'):\n",
        "              iteration_schedule+=1\n",
        "            \n",
        "            # Tell it we are in training mode\n",
        "            net.train()\n",
        "\n",
        "            # Reset all gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Transfer data to GPU\n",
        "            x = torch.tensor(x).to(device)\n",
        "            y = torch.tensor(y).to(device)\n",
        "\n",
        "            logits, (state_h, state_c) = net(x, (state_h, state_c))\n",
        "            loss = criterion(logits.transpose(1, 2), y)\n",
        "\n",
        "            state_h = state_h.detach()\n",
        "            state_c = state_c.detach()\n",
        "\n",
        "            loss_value = loss.item()        \n",
        "\n",
        "            # Perform back-propagation\n",
        "            loss.backward()\n",
        "\n",
        "            _ = torch.nn.utils.clip_grad_norm_(\n",
        "                net.parameters(), flags.gradients_norm)\n",
        "            \n",
        "            # Update the network's parameters\n",
        "            optimizer.step()\n",
        "\n",
        "            #Scheduler step\n",
        "            if (flags.schedule_on=='Y'):\n",
        "              scheduler.step()\n",
        "\n",
        "\n",
        "            if iteration % 100 == 0:\n",
        "                print('Epoch: {}/{}'.format(e, flags.total_epochs), \n",
        "                      'Iteration: {}'.format(iteration),\n",
        "                      '\\t Loss: {}'.format(loss_value))\n",
        "                      #'\\t Perplexity: {}'.format(2**loss_value))\n",
        "        \n",
        "\n",
        "        logits, (state_h, state_c) = net(x, (state_h, state_c))\n",
        "        loss = criterion(logits.transpose(1, 2), y)\n",
        "        loss_value_final = loss.item()    \n",
        "        all_train_losses.append(loss_value_final) \n",
        "        print('Final training loss {}'.format(loss_value_final)) \n",
        "        \n",
        "\n",
        "        ##################### VALIDATION EPOCH ########################\n",
        "        iteration = 0\n",
        "\n",
        "        state_h, state_c = net.zero_state(flags.batch_size)\n",
        "        # Transfer data to GPU\n",
        "\n",
        "        state_h = state_h.to(device)\n",
        "        state_c = state_c.to(device)\n",
        "\n",
        "        for x, y in valid_batches:\n",
        "\n",
        "            iteration += 1\n",
        "\n",
        "            # Tell it we are in evaluation mode\n",
        "            net.eval()\n",
        "\n",
        "            # Transfer data to GPU\n",
        "            x = torch.tensor(x).to(device)\n",
        "            y = torch.tensor(y).to(device)\n",
        "\n",
        "            logits, (state_h, state_c) = net(x, (state_h, state_c))\n",
        "            valid_loss = criterion(logits.transpose(1, 2), y)\n",
        "\n",
        "            state_h = state_h.detach()\n",
        "            state_c = state_c.detach()\n",
        "\n",
        "            valid_loss_value = valid_loss.item()\n",
        "\n",
        "        all_valid_losses.append(valid_loss_value)\n",
        "\n",
        "\n",
        "        ##################### TEST EPOCH ########################\n",
        "        iteration = 0\n",
        "\n",
        "        state_h, state_c = net.zero_state(flags.batch_size)\n",
        "        # Transfer data to GPU\n",
        "\n",
        "        state_h = state_h.to(device)\n",
        "        state_c = state_c.to(device)\n",
        "\n",
        "        for x, y in test_batches:\n",
        "\n",
        "            iteration += 1\n",
        "\n",
        "            # Tell it we are in evaluation mode\n",
        "            net.eval()\n",
        "\n",
        "            # Transfer data to GPU\n",
        "            x = torch.tensor(x).to(device)\n",
        "            y = torch.tensor(y).to(device)\n",
        "\n",
        "            logits, (state_h, state_c) = net(x, (state_h, state_c))\n",
        "            test_loss = criterion(logits.transpose(1, 2), y)\n",
        "\n",
        "            state_h = state_h.detach()\n",
        "            state_c = state_c.detach()\n",
        "\n",
        "            test_loss_value = test_loss.item()\n",
        "\n",
        "        all_test_losses.append(test_loss_value)\n",
        "\n",
        "\n",
        "        print('')\n",
        "       # print('Current mean of validation loss over batches: {}'.format(mean_all_valid_losses))\n",
        "        print('Current validation loss: {}'.format(valid_loss_value))\n",
        "        print('Validation perplexity: {}'.format(np.exp((valid_loss_value))))\n",
        "        print(' ')\n",
        "        print('Current test loss: {}'.format(test_loss_value))\n",
        "        print('Test perplexity: {}'.format(np.exp((test_loss_value))))\n",
        "\n",
        "        print('')\n",
        "            \n",
        "        plt.figure(1)                             \n",
        "        plt.xlabel('Epochs'), plt.ylabel('Loss')   \n",
        "        plt.plot(epochnumber, all_train_losses, 'r', epochnumber, all_valid_losses, 'b', epochnumber, all_test_losses, 'g')\n",
        "        plt.legend(['Train Loss','Validation Loss', 'Test Loss'])    \n",
        "   \n",
        "        last= predict(device, net, flags.initial_words_train, n_vocab,\n",
        "                            vocab_to_int, int_to_vocab, flags.predict_top_k)\n",
        "        print('\\nThe last predicted 200 characters are:\\n')\n",
        "        print(last[-200:])\n",
        "\n",
        "    #path = \"/content/drive/My Drive/DL_project/\"\n",
        "    # save the pretrained model if the validation loss is less than minimum validation loss.\n",
        "        if flags.mode == 'pretraining' and e > 0 and valid_loss_value <= min(all_valid_losses) :\n",
        "          if ( datafrom=='Drive'):\n",
        "            torch.save(net.state_dict(), '/content/drive/My Drive/net.pth')\n",
        "            print('Saved model')\n",
        "          else:\n",
        "            torch.save(net.state_dict(), '/content/drive/My Drive/net.pth')\n",
        "            print('Saved model')\n",
        "\n",
        "\n",
        "    if (flags.mode=='friends'):\n",
        "      print('-----------------------------------------------------------')\n",
        "      print('--------------------Making T-SNE plots---------------------')\n",
        "      print('-----------------------------------------------------------')\n",
        "  \n",
        "\n",
        "      embedding_tensor=net.embedding.weight\n",
        "    \n",
        "      # access Variable's tensor, copy back to CPU, convert to numpy\n",
        "      arr = embedding_tensor.data.cpu().numpy()\n",
        "      # write CSV\n",
        "      #np.savetxt('Embedded_4.csv', arr)\n",
        "      #files.download('Embedded_4.csv')\n",
        "\n",
        "      print(\"Run Y = tsne.tsne(X, no_dims, perplexity) to perform t-SNE on your dataset.\")\n",
        "      print(\"Running example on Friends Data...\")\n",
        "      #X = np.loadtxt(\"Embedded_4.csv\")\n",
        "      X = arr\n",
        "      X = X[0:1000,:] #only the 1040 most used\n",
        "      #with open('Embedded_labels4.csv', 'rb') as f:\n",
        "      #    labels = pickle.load(f)\n",
        "      #labels = labels[0:1000] #only the 1040 most used\n",
        "      labels = list(int_to_vocab.values())[0:1000] #only the 1040 most used\n",
        "                \n",
        "      #labels = np.loadtxt(\"mnist2500_labels.txt\")\n",
        "      Y = tsne(X, 2, 50, 20.0)\n",
        "      #pylab.scatter(Y[:, 0], Y[:, 1], 20, labels)  \n",
        "      #pylab.show()  \n",
        "      \n",
        "      # write labels of interest\n",
        "      subset =['monica','rachel','chandler','ross','joey','phoebe','coffee','gunther','table','muffin','running','drinking','sex','science','movies','beer','pizza','food','janice','guy','girl','wine','paul','barry','it','i','you','he','she','they','ok','love','like','kiss','dinner','lasagna','drink','boy','man','dude','honey','sweetie','hey','hi','hello','muffin','order','place','right','good','fine','so','well']    \n",
        "      #subset = ['man','boy','woman','girl','drink','drinking','eat','eating','love','loving','where','who','what','why','when']\n",
        "      a = 1\n",
        "      labels2 = list()\n",
        "      X2 =list()\n",
        "      Y2 =list()\n",
        "      \n",
        "      \n",
        "      for i in range(1,len(labels)):\n",
        "          if labels[i] in subset:\n",
        "              labels2.append(labels[i])\n",
        "              X2.append(Y[:, 0][i])\n",
        "              Y2.append(Y[:, 1][i])\n",
        "              \n",
        "      plt.figure(2)\n",
        "      #with subset\n",
        "      for i,type in enumerate(labels2):\n",
        "          x = X2[i]\n",
        "          y = Y2[i]\n",
        "          plt.scatter(x, y, marker='o', color='blue')\n",
        "          plt.text(x+0.9, y+0.9, type, fontsize=9)\n",
        "      plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZE35gpjOaFa",
        "colab_type": "text"
      },
      "source": [
        "##Define a prediction function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6dnXz8s6BSI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(device, net, words, n_vocab, vocab_to_int, int_to_vocab, top_k):\n",
        "    net.eval()\n",
        "\n",
        "    state_h, state_c = net.zero_state(1)\n",
        "    state_h = state_h.to(device)\n",
        "    state_c = state_c.to(device)\n",
        "    for w in words:\n",
        "        ix = torch.tensor([[vocab_to_int[w]]]).to(device)\n",
        "        output, (state_h, state_c) = net(ix, (state_h, state_c))\n",
        "    \n",
        "\n",
        "    _, top_ix = torch.topk(output[0], k=top_k)\n",
        "    choices = top_ix.tolist()\n",
        "    choice = np.random.choice(choices[0]) #A way to avoid always choose like \"and\" \"then\"..... \n",
        "\n",
        "    words.append(int_to_vocab[choice])\n",
        "\n",
        "    for _ in range(100):\n",
        "        ix = torch.tensor([[choice]]).to(device)\n",
        "        output, (state_h, state_c) = net(ix, (state_h, state_c))\n",
        "\n",
        "        _, top_ix = torch.topk(output[0], k=top_k)\n",
        "        choices = top_ix.tolist()\n",
        "        choice = np.random.choice(choices[0])\n",
        "        words.append(int_to_vocab[choice])\n",
        "\n",
        "    print(' '.join(words))\n",
        "    return ' '.join(words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zt1sJj100Bx9",
        "colab_type": "text"
      },
      "source": [
        "## Importing T-SNE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72BZeL2P0MjR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#\n",
        "#  tsne.py\n",
        "#\n",
        "# Implementation of t-SNE in Python. The implementation was tested on Python\n",
        "# 2.7.10, and it requires a working installation of NumPy. The implementation\n",
        "# comes with an example on the MNIST dataset. In order to plot the\n",
        "# results of this example, a working installation of matplotlib is required.\n",
        "#\n",
        "# The example can be run by executing: `ipython tsne.py`\n",
        "#\n",
        "#\n",
        "#  Created by Laurens van der Maaten on 20-12-08.\n",
        "#  Copyright (c) 2008 Tilburg University. All rights reserved.\n",
        "\n",
        "import numpy as np\n",
        "import pylab\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "\n",
        "\n",
        "\n",
        "def Hbeta(D=np.array([]), beta=1.0):\n",
        "    \"\"\"\n",
        "        Compute the perplexity and the P-row for a specific value of the\n",
        "        precision of a Gaussian distribution.\n",
        "    \"\"\"\n",
        "\n",
        "    # Compute P-row and corresponding perplexity\n",
        "    P = np.exp(-D.copy() * beta)\n",
        "    sumP = sum(P)\n",
        "    H = np.log(sumP) + beta * np.sum(D * P) / sumP\n",
        "    P = P / sumP\n",
        "    return H, P\n",
        "\n",
        "\n",
        "def x2p(X=np.array([]), tol=1e-5, perplexity=30.0):\n",
        "    \"\"\"\n",
        "        Performs a binary search to get P-values in such a way that each\n",
        "        conditional Gaussian has the same perplexity.\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize some variables\n",
        "    print(\"Computing pairwise distances...\")\n",
        "    (n, d) = X.shape\n",
        "    sum_X = np.sum(np.square(X), 1)\n",
        "    D = np.add(np.add(-2 * np.dot(X, X.T), sum_X).T, sum_X)\n",
        "    P = np.zeros((n, n))\n",
        "    beta = np.ones((n, 1))\n",
        "    logU = np.log(perplexity)\n",
        "\n",
        "    # Loop over all datapoints\n",
        "    for i in range(n):\n",
        "\n",
        "        # Print progress\n",
        "        if i % 500 == 0:\n",
        "            print(\"Computing P-values for point %d of %d...\" % (i, n))\n",
        "\n",
        "        # Compute the Gaussian kernel and entropy for the current precision\n",
        "        betamin = -np.inf\n",
        "        betamax = np.inf\n",
        "        Di = D[i, np.concatenate((np.r_[0:i], np.r_[i+1:n]))]\n",
        "        (H, thisP) = Hbeta(Di, beta[i])\n",
        "\n",
        "        # Evaluate whether the perplexity is within tolerance\n",
        "        Hdiff = H - logU\n",
        "        tries = 0\n",
        "        while np.abs(Hdiff) > tol and tries < 50:\n",
        "\n",
        "            # If not, increase or decrease precision\n",
        "            if Hdiff > 0:\n",
        "                betamin = beta[i].copy()\n",
        "                if betamax == np.inf or betamax == -np.inf:\n",
        "                    beta[i] = beta[i] * 2.\n",
        "                else:\n",
        "                    beta[i] = (beta[i] + betamax) / 2.\n",
        "            else:\n",
        "                betamax = beta[i].copy()\n",
        "                if betamin == np.inf or betamin == -np.inf:\n",
        "                    beta[i] = beta[i] / 2.\n",
        "                else:\n",
        "                    beta[i] = (beta[i] + betamin) / 2.\n",
        "\n",
        "            # Recompute the values\n",
        "            (H, thisP) = Hbeta(Di, beta[i])\n",
        "            Hdiff = H - logU\n",
        "            tries += 1\n",
        "\n",
        "        # Set the final row of P\n",
        "        P[i, np.concatenate((np.r_[0:i], np.r_[i+1:n]))] = thisP\n",
        "\n",
        "    # Return final P-matrix\n",
        "    print(\"Mean value of sigma: %f\" % np.mean(np.sqrt(1 / beta)))\n",
        "    return P\n",
        "\n",
        "\n",
        "def pca(X=np.array([]), no_dims=50):\n",
        "    \"\"\"\n",
        "        Runs PCA on the NxD array X in order to reduce its dimensionality to\n",
        "        no_dims dimensions.\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"Preprocessing the data using PCA...\")\n",
        "    (n, d) = X.shape\n",
        "    X = X - np.tile(np.mean(X, 0), (n, 1))\n",
        "    (l, M) = np.linalg.eig(np.dot(X.T, X))\n",
        "    Y = np.dot(X, M[:, 0:no_dims])\n",
        "    return Y\n",
        "\n",
        "\n",
        "def tsne(X=np.array([]), no_dims=2, initial_dims=50, perplexity=30.0):\n",
        "    \"\"\"\n",
        "        Runs t-SNE on the dataset in the NxD array X to reduce its\n",
        "        dimensionality to no_dims dimensions. The syntaxis of the function is\n",
        "        `Y = tsne.tsne(X, no_dims, perplexity), where X is an NxD NumPy array.\n",
        "    \"\"\"\n",
        "\n",
        "    # Check inputs\n",
        "    if isinstance(no_dims, float):\n",
        "        print(\"Error: array X should have type float.\")\n",
        "        return -1\n",
        "    if round(no_dims) != no_dims:\n",
        "        print(\"Error: number of dimensions should be an integer.\")\n",
        "        return -1\n",
        "\n",
        "    # Initialize variables\n",
        "    X = pca(X, initial_dims).real\n",
        "    (n, d) = X.shape\n",
        "    max_iter = 1000\n",
        "    initial_momentum = 0.5\n",
        "    final_momentum = 0.8\n",
        "    eta = 500\n",
        "    min_gain = 0.01\n",
        "    Y = np.random.randn(n, no_dims)\n",
        "    dY = np.zeros((n, no_dims))\n",
        "    iY = np.zeros((n, no_dims))\n",
        "    gains = np.ones((n, no_dims))\n",
        "\n",
        "    # Compute P-values\n",
        "    P = x2p(X, 1e-5, perplexity)\n",
        "    P = P + np.transpose(P)\n",
        "    P = P / np.sum(P)\n",
        "    P = P * 4.\t\t\t\t\t\t\t\t\t# early exaggeration\n",
        "    P = np.maximum(P, 1e-12)\n",
        "\n",
        "    # Run iterations\n",
        "    for iter in range(max_iter):\n",
        "\n",
        "        # Compute pairwise affinities\n",
        "        sum_Y = np.sum(np.square(Y), 1)\n",
        "        num = -2. * np.dot(Y, Y.T)\n",
        "        num = 1. / (1. + np.add(np.add(num, sum_Y).T, sum_Y))\n",
        "        num[range(n), range(n)] = 0.\n",
        "        Q = num / np.sum(num)\n",
        "        Q = np.maximum(Q, 1e-12)\n",
        "\n",
        "        # Compute gradient\n",
        "        PQ = P - Q\n",
        "        for i in range(n):\n",
        "            dY[i, :] = np.sum(np.tile(PQ[:, i] * num[:, i], (no_dims, 1)).T * (Y[i, :] - Y), 0)\n",
        "\n",
        "        # Perform the update\n",
        "        if iter < 20:\n",
        "            momentum = initial_momentum\n",
        "        else:\n",
        "            momentum = final_momentum\n",
        "        gains = (gains + 0.2) * ((dY > 0.) != (iY > 0.)) + \\\n",
        "                (gains * 0.8) * ((dY > 0.) == (iY > 0.))\n",
        "        gains[gains < min_gain] = min_gain\n",
        "        iY = momentum * iY - eta * (gains * dY)\n",
        "        Y = Y + iY\n",
        "        Y = Y - np.tile(np.mean(Y, 0), (n, 1))\n",
        "\n",
        "        # Compute current value of cost function\n",
        "        if (iter + 1) % 10 == 0:\n",
        "            C = np.sum(P * np.log(P / Q))\n",
        "            print(\"Iteration %d: error is %f\" % (iter + 1, C))\n",
        "\n",
        "        # Stop lying about P-values\n",
        "        if iter == 100:\n",
        "            P = P / 4.\n",
        "\n",
        "    # Return solution\n",
        "    return Y\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rp4Z_Us8OfT6",
        "colab_type": "text"
      },
      "source": [
        "#***Run model***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKG_14IA6JCF",
        "colab_type": "code",
        "outputId": "c11f4b76-0f15-4983-d881-d2d33bd206ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    main()\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------  \n",
            "----------------------Go into training mode--------------------------------  \n",
            "----------------------using friends data------------------------\n",
            "------------------------------------------------------------------------  \n",
            "Vocabulary size 33347\n",
            "Epoch: 0/2 Iteration: 100 \t Loss: 6.43353796005249\n",
            "Epoch: 0/2 Iteration: 200 \t Loss: 5.885072231292725\n",
            "Epoch: 0/2 Iteration: 300 \t Loss: 5.754425525665283\n",
            "Epoch: 0/2 Iteration: 400 \t Loss: 5.4175567626953125\n",
            "Epoch: 0/2 Iteration: 500 \t Loss: 5.382619380950928\n",
            "Epoch: 0/2 Iteration: 600 \t Loss: 5.365413188934326\n",
            "Epoch: 0/2 Iteration: 700 \t Loss: 5.09623908996582\n",
            "Epoch: 0/2 Iteration: 800 \t Loss: 5.304572582244873\n",
            "Epoch: 0/2 Iteration: 900 \t Loss: 5.020531177520752\n",
            "Epoch: 0/2 Iteration: 1000 \t Loss: 5.044727325439453\n",
            "Epoch: 0/2 Iteration: 1100 \t Loss: 4.879377365112305\n",
            "Epoch: 0/2 Iteration: 1200 \t Loss: 4.94788122177124\n",
            "Epoch: 0/2 Iteration: 1300 \t Loss: 4.753312110900879\n",
            "Epoch: 0/2 Iteration: 1400 \t Loss: 4.704807281494141\n",
            "Epoch: 0/2 Iteration: 1500 \t Loss: 5.163300037384033\n",
            "Epoch: 0/2 Iteration: 1600 \t Loss: 4.856583595275879\n",
            "Epoch: 0/2 Iteration: 1700 \t Loss: 4.828240394592285\n",
            "Final training loss 4.71836519241333\n",
            "\n",
            "Current validation loss: 5.050256729125977\n",
            "Validation perplexity: 156.06252513948348\n",
            " \n",
            "Current test loss: 4.839270114898682\n",
            "Test perplexity: 126.37707731347102\n",
            "\n",
            "going is a time , he has it and chandler . ] rachel: hey ? ! joey: yeah . monica: ( he to him , chandler , the couch is the phone . the <unk> ) ( he is to leave . he is to get a head , he goes to her the door of his door ) and i can't have to do you to the lot . monica: yeah , no ! ( she looks . ) ( they her to him ) hey . rachel: ( chandler to joey and rachel to monica . he goes to her\n",
            "\n",
            "The last predicted 200 characters are:\n",
            "\n",
            "oes to her the door of his door ) and i can't have to do you to the lot . monica: yeah , no ! ( she looks . ) ( they her to him ) hey . rachel: ( chandler to joey and rachel to monica . he goes to her\n",
            "Epoch: 1/2 Iteration: 300 \t Loss: 4.819886684417725\n",
            "Epoch: 1/2 Iteration: 400 \t Loss: 4.823408603668213\n",
            "Epoch: 1/2 Iteration: 500 \t Loss: 4.78032922744751\n",
            "Epoch: 1/2 Iteration: 600 \t Loss: 4.684825897216797\n",
            "Epoch: 1/2 Iteration: 700 \t Loss: 4.788895130157471\n",
            "Epoch: 1/2 Iteration: 800 \t Loss: 4.671502590179443\n",
            "Final training loss 4.785885334014893\n",
            "\n",
            "Current validation loss: 4.7634124755859375\n",
            "Validation perplexity: 117.14499904729888\n",
            " \n",
            "Current test loss: 4.767096519470215\n",
            "Test perplexity: 117.5773622981987\n",
            "\n",
            "going is a time , he has it and chandler . ] rachel: hey ? ! joey: yeah . monica: ( he to him , chandler , the couch is the phone . the <unk> ) ( he is to leave . he is to get a head , he goes to her the door of his door ) and i can't have to do you to the lot . monica: yeah , no ! ( she looks . ) ( they her to him ) hey . rachel: ( chandler to joey and rachel to monica . he goes to her . ) monica: hey ! joey: yeah . ( to rachel . the phone and chandler starts the bathroom ) monica: i mean , i'm not . ( chandler goes , and he looks the <unk> , he goes the door and chandler . ) ross: i - i'm just know it ! i can't know ! ( he starts a <unk> of his phone and rachel to leave . he has to the <unk> , ross has it on the couch and ross is it and chandler are the <unk> and phoebe to get to her and the couch to\n",
            "\n",
            "The last predicted 200 characters are:\n",
            "\n",
            "i can't know ! ( he starts a <unk> of his phone and rachel to leave . he has to the <unk> , ross has it on the couch and ross is it and chandler are the <unk> and phoebe to get to her and the couch to\n",
            "-----------------------------------------------------------\n",
            "--------------------Making T-SNE plots---------------------\n",
            "-----------------------------------------------------------\n",
            "Run Y = tsne.tsne(X, no_dims, perplexity) to perform t-SNE on your dataset.\n",
            "Running example on Friends Data...\n",
            "Preprocessing the data using PCA...\n",
            "Computing pairwise distances...\n",
            "Computing P-values for point 0 of 1000...\n",
            "Computing P-values for point 500 of 1000...\n",
            "Mean value of sigma: 2.208266\n",
            "Iteration 10: error is 20.099041\n",
            "Iteration 20: error is 20.229759\n",
            "Iteration 30: error is 20.700403\n",
            "Iteration 40: error is 21.077683\n",
            "Iteration 50: error is 21.588646\n",
            "Iteration 60: error is 21.336391\n",
            "Iteration 70: error is 20.940638\n",
            "Iteration 80: error is 21.356240\n",
            "Iteration 90: error is 21.403706\n",
            "Iteration 100: error is 21.177565\n",
            "Iteration 110: error is 3.655917\n",
            "Iteration 120: error is 3.310357\n",
            "Iteration 130: error is 3.179363\n",
            "Iteration 140: error is 3.103725\n",
            "Iteration 150: error is 3.044172\n",
            "Iteration 160: error is 2.995346\n",
            "Iteration 170: error is 2.952533\n",
            "Iteration 180: error is 2.910613\n",
            "Iteration 190: error is 2.861412\n",
            "Iteration 200: error is 2.807642\n",
            "Iteration 210: error is 2.777759\n",
            "Iteration 220: error is 2.765396\n",
            "Iteration 230: error is 2.755301\n",
            "Iteration 240: error is 2.747002\n",
            "Iteration 250: error is 2.739293\n",
            "Iteration 260: error is 2.732173\n",
            "Iteration 270: error is 2.726155\n",
            "Iteration 280: error is 2.720532\n",
            "Iteration 290: error is 2.715596\n",
            "Iteration 300: error is 2.711029\n",
            "Iteration 310: error is 2.706682\n",
            "Iteration 320: error is 2.703388\n",
            "Iteration 330: error is 2.700729\n",
            "Iteration 340: error is 2.698340\n",
            "Iteration 350: error is 2.696106\n",
            "Iteration 360: error is 2.694104\n",
            "Iteration 370: error is 2.692364\n",
            "Iteration 380: error is 2.690728\n",
            "Iteration 390: error is 2.689168\n",
            "Iteration 400: error is 2.687663\n",
            "Iteration 410: error is 2.685971\n",
            "Iteration 420: error is 2.684482\n",
            "Iteration 430: error is 2.683439\n",
            "Iteration 440: error is 2.682544\n",
            "Iteration 450: error is 2.681663\n",
            "Iteration 460: error is 2.680396\n",
            "Iteration 470: error is 2.679373\n",
            "Iteration 480: error is 2.678547\n",
            "Iteration 490: error is 2.677799\n",
            "Iteration 500: error is 2.677088\n",
            "Iteration 510: error is 2.676089\n",
            "Iteration 520: error is 2.674817\n",
            "Iteration 530: error is 2.674090\n",
            "Iteration 540: error is 2.673550\n",
            "Iteration 550: error is 2.673129\n",
            "Iteration 560: error is 2.672793\n",
            "Iteration 570: error is 2.672514\n",
            "Iteration 580: error is 2.672249\n",
            "Iteration 590: error is 2.671983\n",
            "Iteration 600: error is 2.671730\n",
            "Iteration 610: error is 2.671476\n",
            "Iteration 620: error is 2.671210\n",
            "Iteration 630: error is 2.670949\n",
            "Iteration 640: error is 2.670674\n",
            "Iteration 650: error is 2.670304\n",
            "Iteration 660: error is 2.670023\n",
            "Iteration 670: error is 2.669802\n",
            "Iteration 680: error is 2.669542\n",
            "Iteration 690: error is 2.669359\n",
            "Iteration 700: error is 2.669191\n",
            "Iteration 710: error is 2.669032\n",
            "Iteration 720: error is 2.668875\n",
            "Iteration 730: error is 2.668718\n",
            "Iteration 740: error is 2.668562\n",
            "Iteration 750: error is 2.668394\n",
            "Iteration 760: error is 2.668128\n",
            "Iteration 770: error is 2.667965\n",
            "Iteration 780: error is 2.667841\n",
            "Iteration 790: error is 2.667728\n",
            "Iteration 800: error is 2.667623\n",
            "Iteration 810: error is 2.667522\n",
            "Iteration 820: error is 2.667422\n",
            "Iteration 830: error is 2.667322\n",
            "Iteration 840: error is 2.667226\n",
            "Iteration 850: error is 2.667130\n",
            "Iteration 860: error is 2.667035\n",
            "Iteration 870: error is 2.666941\n",
            "Iteration 880: error is 2.666850\n",
            "Iteration 890: error is 2.666763\n",
            "Iteration 900: error is 2.666678\n",
            "Iteration 910: error is 2.666588\n",
            "Iteration 920: error is 2.666434\n",
            "Iteration 930: error is 2.666312\n",
            "Iteration 940: error is 2.666209\n",
            "Iteration 950: error is 2.666117\n",
            "Iteration 960: error is 2.666026\n",
            "Iteration 970: error is 2.665940\n",
            "Iteration 980: error is 2.665861\n",
            "Iteration 990: error is 2.665782\n",
            "Iteration 1000: error is 2.665703\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEGCAYAAAB/+QKOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngCxBGGOMCcgShDHGmID+HyGxbtKsA39DAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD4CAYAAADrRI2NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nwmSfGYpcsgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdofkAfbsgAt",
        "colab_type": "text"
      },
      "source": [
        "Extract word labels\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXhiKEWCse4k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#text=flags.train_file_friends.split()\n",
        "text=flags.train_file_pretraining.split()\n",
        "   # Extend words_notinpretraining to text to get them as a part of the mapping dictionary\n",
        "text.extend(words_notinpretraining)\n",
        "word_counts = Counter(text)\n",
        "sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
        "int_to_vocab = {k: w for k, w in enumerate(sorted_vocab)}\n",
        "vocab_to_int = {w: k for k, w in int_to_vocab.items()}\n",
        "n_vocab = len(int_to_vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SyM0Vt7Usq9i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import pickle\n",
        "#with open(\"Embedded_labels4.csv\", \"wb\") as fp:   #Pickling\n",
        "#  pickle.dump(sorted_vocab, fp)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxgZnC0VGw2u",
        "colab_type": "code",
        "outputId": "e16ba232-c3f6-4382-af38-61a8dbe44e4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "n_vocab"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "33347"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7xjhSeKstMH",
        "colab_type": "text"
      },
      "source": [
        "Extract embedding weights\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJgBgiw27RiR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net = RNNModule(n_vocab, flags.seq_size,\n",
        "                    flags.embedding_size, flags.lstm_size)\n",
        "\n",
        "net.load_state_dict(torch.load('/content/drive/My Drive/net.pth'))\n",
        "device='cpu'\n",
        "net = net.to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IyNKLTlg8VZI",
        "colab_type": "code",
        "outputId": "2d9cfb25-406f-437e-9776-9e885cc0a968",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "net.embedding.weight"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[-0.6379,  1.0463, -0.3401,  ..., -0.4577, -0.3466, -0.0074],\n",
              "        [-1.3347,  0.6889,  0.4081,  ...,  0.0594, -0.5343,  0.2030],\n",
              "        [ 0.6943, -0.9482, -0.7068,  ...,  1.3558, -1.3177,  0.0564],\n",
              "        ...,\n",
              "        [ 0.8853,  0.3069, -0.8586,  ..., -0.1304,  0.4129, -1.5304],\n",
              "        [-0.1783, -0.0155,  0.7481,  ...,  0.7634, -0.0789, -0.9984],\n",
              "        [-1.0535, -0.6745, -1.0471,  ...,  0.1844, -0.9779, -0.4697]],\n",
              "       requires_grad=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    }
  ]
}