{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Friends_Generator.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LauJohansson/DeepLearning_NLP_Friends/blob/master/Friends_Generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2rMr8O2cFut",
        "colab_type": "text"
      },
      "source": [
        "#How to use"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5IyCeet2cKEX",
        "colab_type": "text"
      },
      "source": [
        "**Press \"runtime\" -> \"run all\"**\n",
        "\n",
        "**In the bottom of this notebook, there will pop up input-boxes. Here's an example of what you can give as inputs:**\n",
        "\n",
        "\n",
        "*Please enter the first word of your Friends manuscript (seperate with space): chandler and ross*\n",
        "\n",
        "*Please enter the number of words that your Friends manuscript should contain: 1000*\n",
        "\n",
        "*Choose a number from 1-100 (if 1, then there is no randomness in word prediction): 5*\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQs3Zti_Xq4A",
        "colab_type": "text"
      },
      "source": [
        "#Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtBQfWMUXw1k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from argparse import Namespace\n",
        "\n",
        "flags = Namespace(\n",
        "    mode = 'target', #'pretraining' is pretraining and 'target' is fine-tuning of either PTB data or friends data. \n",
        "    name='Friends',\n",
        "    seq_size= 32,                        #Sequence Length\n",
        "    batch_size=40,                       #Batch size\n",
        "    embedding_size=256,                  #Embedding size\n",
        "    lstm_size=512,                       #Hidden nodes size\n",
        "    gradients_norm=0.5,   \n",
        "    #initial_words=['banknote', 'berlitz'],\n",
        "    initial_words_train = [],\n",
        "    initial_words_valid= [],\n",
        "    predict_top_k=5,                    #Choose the k best next_word_prediction, and a random is chosing.\n",
        "    checkpoint_path='checkpoint',\n",
        "    total_epochs=100,                   #Choose number of epochs in training\n",
        "    learning_rate=0.001,                #Choose number of epochs in training\n",
        "    predict_every=1000,\n",
        "    #validation_corpus_size=len(valid_file.split()),\n",
        "    dropconnect_rate=0.4,               #Choose drop connect rate\n",
        "    n_lay=2,                            #Choose number of LSTM layers\n",
        "\n",
        "    #Set variational sequence length on/off\n",
        "    var_seq='Y',                            #Choose if variational sequence length is on/off\n",
        "    var_seq_std=2,                          #Choose std. dev. for norm distribution for var. seq. length ( in moment 1/2 of seq length)\n",
        "\n",
        "\n",
        "    #scheduler parameters\n",
        "    schedule_on='N',                        #Choose if LR-scheduler is on/off\n",
        "    triangular='N',                         #Choose 'Y' to turn on the slanted triangular LR. \n",
        "    cut_fracI=0.2,                          #Choose the fraction of iterations we increase the LR in STL\n",
        "    ratioI=32,                              #Choose how much smaller the lowest LR is from the maximum LR Î·max\n",
        "    nmaxI=0.0,                              #this will be set = learning_rate \n",
        "\n",
        "    #Use same drop-mask for drop-connect\n",
        "    same_drop_lstm='N',                     #Choose 'Y' if drop-connect all should use same mask\n",
        "    \n",
        "    #Dropout on embedding layer\n",
        "    drop_embed=0.5,                         #Choose dropoutrate for embedding dropout        \n",
        "\n",
        "    #Optimizer selection\n",
        "    optim_select='AdamW'                      #Choose between \"AdamW, SGD, ASGD\"\n",
        "\n",
        ")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IhaW_SmSzWgj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_data_from_file(train_file, batch_size, seq_size):\n",
        "  \n",
        "    text=train_file.split()\n",
        "    # Extend words_notinpretraining to text to get them as a part of the mapping dictionary\n",
        "    #text.extend(words_notinpretraining)\n",
        "    word_counts = Counter(text)\n",
        "    sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
        "    int_to_vocab = {k: w for k, w in enumerate(sorted_vocab)}\n",
        "    vocab_to_int = {w: k for k, w in int_to_vocab.items()}\n",
        "    n_vocab = len(int_to_vocab)\n",
        "\n",
        "    text=train_file.split()\n",
        "\n",
        "    print('Vocabulary size', n_vocab)\n",
        "\n",
        "    int_text = [vocab_to_int[w] for w in text]\n",
        "    num_batches = int(len(int_text) / (seq_size * batch_size))\n",
        "    in_text = int_text[:num_batches * batch_size * seq_size]\n",
        "    out_text = np.zeros_like(in_text)\n",
        "    out_text[:-1] = in_text[1:]\n",
        "    out_text[-1] = in_text[0]\n",
        "    in_text = np.reshape(in_text, (batch_size, -1))\n",
        "    out_text = np.reshape(out_text, (batch_size, -1))\n",
        "    return int_to_vocab, vocab_to_int, n_vocab, in_text, out_text, sorted_vocab"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Agm7zSvWzYoX",
        "colab_type": "code",
        "outputId": "199fd20b-f2b5-40a6-e1ed-b6cc973a19c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from urllib.request import urlopen\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "data = str(urlopen('https://raw.githubusercontent.com/LauJohansson/DeepLearning_NLP_Friends/master/Data/friends_train.txt').read(),encoding=\"utf-8\")\n",
        "\n",
        "int_to_vocab, vocab_to_int, n_vocab, in_text, out_text,sorted_vocab = get_data_from_file( \n",
        "          data, flags.batch_size, flags.seq_size)\n",
        "      \n",
        "\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary size 11431\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXoJ0zOClMTe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RNNModule(nn.Module):\n",
        "    def __init__(self, n_vocab, seq_size, embedding_size, lstm_size):\n",
        "        super(RNNModule, self).__init__()\n",
        "        self.seq_size = seq_size\n",
        "        self.lstm_size = lstm_size\n",
        "        self.embedding = nn.Embedding(n_vocab, embedding_size)\n",
        "        self.lstm=nn.LSTM(embedding_size,\n",
        "                            lstm_size,\n",
        "                            batch_first=True,num_layers=flags.n_lay)\n",
        "        \n",
        "        self.dense = nn.Linear(lstm_size, n_vocab)\n",
        "        self.drop_out=nn.Dropout(flags.drop_embed)\n",
        "        self.OneMaskOnly=torch.autograd.Variable(torch.Tensor(getattr(self.lstm,self.lstm._all_weights[0][0]).shape[0],\n",
        "                                                        getattr(self.lstm,self.lstm._all_weights[0][0]).shape[1]).uniform_().to(\"cuda\") > flags.dropconnect_rate)\n",
        "\n",
        "\n",
        "    def forward(self, x, prev_state):\n",
        "        embed = self.embedding(x)\n",
        "        embed=self.drop_out(embed)\n",
        "        orig=[]\n",
        "\n",
        "        #Make dropconnect\n",
        "        if self.training:\n",
        "          for i in range( len(self.lstm._all_weights[0])):\n",
        "            name = self.lstm._all_weights[0][i]\n",
        "            if name.find('LSTM.weight_hh_l')!=-1:      \n",
        "              orig.append(getattr(self.lstm,name))\n",
        "            \n",
        "              if flags.same_drop_lstm=='Y':\n",
        "                mask=self.OneMaskOnly\n",
        "              else:\n",
        "                mask=torch.autograd.Variable(torch.Tensor(getattr(self.lstm,name).shape[0],\n",
        "                                                        getattr(self.lstm,name).shape[1]).uniform_().to(\"cuda\") > flags.flags.dropconnect_rate)\n",
        "              setattr(self.lstm,name,torch.nn.Parameter(torch.mul(getattr(self.lstm,name),mask)))\n",
        "              \n",
        "              self.lstm.flatten_parameters()\n",
        "             \n",
        "\n",
        "        #LSTM forward\n",
        "        output, state = self.lstm(embed, prev_state)\n",
        "\n",
        "        #Set hh-weight back to original\n",
        "        if self.training:\n",
        "          a=0\n",
        "          for i in range( len(self.lstm._all_weights[0])):\n",
        "            name = self.lstm._all_weights[0][i]\n",
        "            if name.find('LSTM.weight_hh_l')!=-1:\n",
        "              print(orig)\n",
        "              setattr(self.lstm,name,orig[a])\n",
        "              #self.lstm.weight_hh_l0=orig\n",
        "              self.lstm.flatten_parameters()\n",
        "              a=+1\n",
        "\n",
        "        logits = self.dense(output)\n",
        "\n",
        "        return logits, state\n",
        "       \n",
        "    def zero_state(self, batch_size):\n",
        "        return (torch.zeros(flags.n_lay, batch_size, self.lstm_size),\n",
        "                torch.zeros(flags.n_lay, batch_size, self.lstm_size))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxcklPI-kB19",
        "colab_type": "code",
        "outputId": "22869b85-d3b3-4e69-ad16-9b41fc787d97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "net = RNNModule(n_vocab, flags.seq_size,\n",
        "                    flags.embedding_size, flags.lstm_size)\n",
        "\n",
        "#net.load_state_dict(torch.load('/content/drive/My Drive/DL_project/net_final.pth'))\n",
        "net_from_git=urlopen('https://raw.githubusercontent.com/LauJohansson/DeepLearning_NLP_Friends/master/Data/net_final_online_ToGenerator_v1.pth').read()\n",
        "\n",
        "net.load_state_dict(torch.load('/content/drive/My Drive/DL_project/net_final_online_ToGenerator_v1.pth'))\n",
        "\n",
        "net = net.to(device)\n",
        "\n",
        "net.eval()\n",
        "\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RNNModule(\n",
              "  (embedding): Embedding(11431, 256)\n",
              "  (lstm): LSTM(256, 512, num_layers=2, batch_first=True)\n",
              "  (dense): Linear(in_features=512, out_features=11431, bias=True)\n",
              "  (drop_out): Dropout(p=0.5, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5ucakDfkAJu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(device, net, words, n_vocab, vocab_to_int, int_to_vocab, top_k,manu_length):\n",
        "    #net.eval()\n",
        "\n",
        "    state_h, state_c = net.zero_state(1)\n",
        "    state_h = state_h.to(device)\n",
        "    state_c = state_c.to(device)\n",
        "    for w in words:\n",
        "        ix = torch.tensor([[vocab_to_int[w]]]).to(device)\n",
        "        output, (state_h, state_c) = net(ix, (state_h, state_c))\n",
        "    \n",
        "\n",
        "    _, top_ix = torch.topk(output[0], k=top_k)\n",
        "    choices = top_ix.tolist()\n",
        "    choice = np.random.choice(choices[0]) #A way to avoid always choose like \"and\" \"then\"..... \n",
        "\n",
        "    words.append(int_to_vocab[choice])\n",
        "\n",
        "    for _ in range(manu_length):\n",
        "        ix = torch.tensor([[choice]]).to(device)\n",
        "        output, (state_h, state_c) = net(ix, (state_h, state_c))\n",
        "\n",
        "        _, top_ix = torch.topk(output[0], k=top_k)\n",
        "        choices = top_ix.tolist()\n",
        "        choice = np.random.choice(choices[0])\n",
        "        words.append(int_to_vocab[choice])\n",
        "\n",
        "    \n",
        "\n",
        "    output= ' '.join(words)#.replace('. ', '.\\n')\n",
        "\n",
        "    output=output.replace('chandler:', '\\nchandler:')\n",
        "    output=output.replace('ross:', '\\nross:')\n",
        "    output=output.replace('joey:', '\\njoey')\n",
        "    output=output.replace('monica', '\\nmonica:')\n",
        "    output=output.replace('phoebe:', '\\nphoebe:')\n",
        "    output=output.replace('rachel:', '\\nrachel:')\n",
        "\n",
        "    \n",
        "    return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SU6yJiC9xCEe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_init_words():\n",
        "  initial_words = input(\"Please enter the first word of your Friends manuscript (seperate with space): \")\n",
        "  inital_words=initial_words.lower()\n",
        "  initial_words=initial_words.split()\n",
        "  initial_words=['[','scene:'] + initial_words\n",
        "  return initial_words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f921DXqP10Ix",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_manuscript():\n",
        "  initial_words=get_init_words()\n",
        "\n",
        "  number_of_words = int(input(\"Please enter the number of words that your Friends manuscript should contain: \"))\n",
        "\n",
        "  randomness = int(input(\"Choose a number from 1-100 (if 1, then there is no randomness in word prediction): \"))\n",
        "\n",
        "  if randomness <1:\n",
        "    randomness=1\n",
        "  if randomness>100:\n",
        "    randomness=100  \n",
        "\n",
        "\n",
        "  #Only print of max 10000 words\n",
        "  if number_of_words>10000:\n",
        "    number_of_words=10000\n",
        "\n",
        "  #Make loop which checks if the entered words are in the vocab\n",
        "  first_loop=1\n",
        "  good_list=0\n",
        "\n",
        "  while good_list==0 or first_loop==1:\n",
        "    first_loop=0\n",
        "    good_list=1\n",
        "    result=all(elem in sorted_vocab for elem in  initial_words)\n",
        "\n",
        "    if not result:\n",
        "      good_list=0\n",
        "    if good_list==0:\n",
        "      print('Some of the words you entered is not a word that is used in Friends')\n",
        "      initial_words=get_init_words()\n",
        "  print('\\n')\n",
        "  print( predict(device, net, initial_words, n_vocab,\n",
        "                            vocab_to_int, int_to_vocab, randomness,number_of_words))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4EXdABCiX0Rm",
        "colab_type": "text"
      },
      "source": [
        "#Generate manuscript:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hsw82MiX2DTp",
        "colab_type": "code",
        "outputId": "17b65d09-8bdf-4031-bb04-df13318edcbf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "generate_manuscript()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Please enter the first word of your Friends manuscript (seperate with space): chandler kicks\n",
            "Please enter the number of words that your Friends manuscript should contain: 100\n",
            "Choose a number from 1-100 (if 1, then there is no randomness in word prediction): 5\n",
            "\n",
            "\n",
            "[ scene: chandler kicks the apartment with the phone on , joey enters with her apartment to find her and sees the chick . ) \n",
            "joey what ? \n",
            "joey hey , you are so sweet ! \n",
            "monica:: oh ! oh , yeah ? ( to the duck and ross ) what are i talking ? \n",
            "chandler: i'm not supposed . i'm just gonna have the same thing , i'm not . \n",
            "joey i know what i did ? \n",
            "monica:: ( to phoebe in tow , and sees joey , chandler , joey ) hey joey , joey and i know you were going to go\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}