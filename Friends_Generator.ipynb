{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Friends_Generator.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LauJohansson/DeepLearning_NLP_Friends/blob/master/Friends_Generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2rMr8O2cFut",
        "colab_type": "text"
      },
      "source": [
        "#How to use"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5IyCeet2cKEX",
        "colab_type": "text"
      },
      "source": [
        "**Press \"runtime\" -> \"run all\"**\n",
        "\n",
        "**In the bottom of this notebook, there will pop up input-boxes. Here's an example of what you can give as inputs:**\n",
        "\n",
        "\n",
        "*Please enter the first word of your Friends manuscript (seperate with space): chandler and ross*\n",
        "\n",
        "*Please enter the number of words that your Friends manuscript should contain: 1000*\n",
        "\n",
        "*Choose a number from 1-100 (if 1, then there is no randomness in word prediction): 5*\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQs3Zti_Xq4A",
        "colab_type": "text"
      },
      "source": [
        "#Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtBQfWMUXw1k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from argparse import Namespace\n",
        "\n",
        "flags = Namespace(\n",
        "    mode = 'target', #'pretraining' is pretraining and 'target' is fine-tuning of either PTB data or friends data. \n",
        "    name='Friends',\n",
        "    seq_size= 32,                        #Sequence Length\n",
        "    batch_size=40,                       #Batch size\n",
        "    embedding_size=256,                  #Embedding size\n",
        "    lstm_size=512,                       #Hidden nodes size\n",
        "    gradients_norm=0.5,   \n",
        "    #initial_words=['banknote', 'berlitz'],\n",
        "    initial_words_train = [],\n",
        "    initial_words_valid= [],\n",
        "    predict_top_k=5,                    #Choose the k best next_word_prediction, and a random is chosing.\n",
        "    checkpoint_path='checkpoint',\n",
        "    total_epochs=100,                   #Choose number of epochs in training\n",
        "    learning_rate=0.001,                #Choose number of epochs in training\n",
        "    predict_every=1000,\n",
        "    #validation_corpus_size=len(valid_file.split()),\n",
        "    dropconnect_rate=0.4,               #Choose drop connect rate\n",
        "    n_lay=2,                            #Choose number of LSTM layers\n",
        "\n",
        "    #Set variational sequence length on/off\n",
        "    var_seq='Y',                            #Choose if variational sequence length is on/off\n",
        "    var_seq_std=2,                          #Choose std. dev. for norm distribution for var. seq. length ( in moment 1/2 of seq length)\n",
        "\n",
        "\n",
        "    #scheduler parameters\n",
        "    schedule_on='N',                        #Choose if LR-scheduler is on/off\n",
        "    triangular='N',                         #Choose 'Y' to turn on the slanted triangular LR. \n",
        "    cut_fracI=0.2,                          #Choose the fraction of iterations we increase the LR in STL\n",
        "    ratioI=32,                              #Choose how much smaller the lowest LR is from the maximum LR Î·max\n",
        "    nmaxI=0.0,                              #this will be set = learning_rate \n",
        "\n",
        "    #Use same drop-mask for drop-connect\n",
        "    same_drop_lstm='N',                     #Choose 'Y' if drop-connect all should use same mask\n",
        "    \n",
        "    #Dropout on embedding layer\n",
        "    drop_embed=0.5,                         #Choose dropoutrate for embedding dropout        \n",
        "\n",
        "    #Optimizer selection\n",
        "    optim_select='AdamW'                      #Choose between \"AdamW, SGD, ASGD\"\n",
        "\n",
        ")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IhaW_SmSzWgj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_data_from_file(train_file, batch_size, seq_size):\n",
        "  \n",
        "    text=train_file.split()\n",
        "    # Extend words_notinpretraining to text to get them as a part of the mapping dictionary\n",
        "    #text.extend(words_notinpretraining)\n",
        "    word_counts = Counter(text)\n",
        "    sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
        "    int_to_vocab = {k: w for k, w in enumerate(sorted_vocab)}\n",
        "    vocab_to_int = {w: k for k, w in int_to_vocab.items()}\n",
        "    n_vocab = len(int_to_vocab)\n",
        "\n",
        "    text=train_file.split()\n",
        "\n",
        "    print('Vocabulary size', n_vocab)\n",
        "\n",
        "    int_text = [vocab_to_int[w] for w in text]\n",
        "    num_batches = int(len(int_text) / (seq_size * batch_size))\n",
        "    in_text = int_text[:num_batches * batch_size * seq_size]\n",
        "    out_text = np.zeros_like(in_text)\n",
        "    out_text[:-1] = in_text[1:]\n",
        "    out_text[-1] = in_text[0]\n",
        "    in_text = np.reshape(in_text, (batch_size, -1))\n",
        "    out_text = np.reshape(out_text, (batch_size, -1))\n",
        "    return int_to_vocab, vocab_to_int, n_vocab, in_text, out_text, sorted_vocab"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Agm7zSvWzYoX",
        "colab_type": "code",
        "outputId": "e13dc5cd-2831-404f-a7e1-14ca795bcd29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from urllib.request import urlopen\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "data = str(urlopen('https://raw.githubusercontent.com/LauJohansson/DeepLearning_NLP_Friends/master/Data/friends_train.txt').read(),encoding=\"utf-8\")\n",
        "\n",
        "int_to_vocab, vocab_to_int, n_vocab, in_text, out_text,sorted_vocab = get_data_from_file( \n",
        "          data, flags.batch_size, flags.seq_size)\n",
        "      \n",
        "\n"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary size 11431\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXoJ0zOClMTe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RNNModule(nn.Module):\n",
        "    def __init__(self, n_vocab, seq_size, embedding_size, lstm_size):\n",
        "        super(RNNModule, self).__init__()\n",
        "        self.seq_size = seq_size\n",
        "        self.lstm_size = lstm_size\n",
        "        self.embedding = nn.Embedding(n_vocab, embedding_size)\n",
        "        self.lstm=nn.LSTM(embedding_size,\n",
        "                            lstm_size,\n",
        "                            batch_first=True,num_layers=flags.n_lay)\n",
        "        \n",
        "        self.dense = nn.Linear(lstm_size, n_vocab)\n",
        "        self.drop_out=nn.Dropout(flags.drop_embed)\n",
        "        self.OneMaskOnly=torch.autograd.Variable(torch.Tensor(getattr(self.lstm,self.lstm._all_weights[0][0]).shape[0],\n",
        "                                                        getattr(self.lstm,self.lstm._all_weights[0][0]).shape[1]).uniform_().to(\"cuda\") > flags.dropconnect_rate)\n",
        "\n",
        "\n",
        "    def forward(self, x, prev_state):\n",
        "        embed = self.embedding(x)\n",
        "        embed=self.drop_out(embed)\n",
        "        orig=[]\n",
        "\n",
        "        #Make dropconnect\n",
        "        if self.training:\n",
        "          for i in range( len(self.lstm._all_weights[0])):\n",
        "            name = self.lstm._all_weights[0][i]\n",
        "            if name.find('LSTM.weight_hh_l')!=-1:      \n",
        "              orig.append(getattr(self.lstm,name))\n",
        "            \n",
        "              if flags.same_drop_lstm=='Y':\n",
        "                mask=self.OneMaskOnly\n",
        "              else:\n",
        "                mask=torch.autograd.Variable(torch.Tensor(getattr(self.lstm,name).shape[0],\n",
        "                                                        getattr(self.lstm,name).shape[1]).uniform_().to(\"cuda\") > flags.flags.dropconnect_rate)\n",
        "              setattr(self.lstm,name,torch.nn.Parameter(torch.mul(getattr(self.lstm,name),mask)))\n",
        "              \n",
        "              self.lstm.flatten_parameters()\n",
        "             \n",
        "\n",
        "        #LSTM forward\n",
        "        output, state = self.lstm(embed, prev_state)\n",
        "\n",
        "        #Set hh-weight back to original\n",
        "        if self.training:\n",
        "          a=0\n",
        "          for i in range( len(self.lstm._all_weights[0])):\n",
        "            name = self.lstm._all_weights[0][i]\n",
        "            if name.find('LSTM.weight_hh_l')!=-1:\n",
        "              print(orig)\n",
        "              setattr(self.lstm,name,orig[a])\n",
        "              #self.lstm.weight_hh_l0=orig\n",
        "              self.lstm.flatten_parameters()\n",
        "              a=+1\n",
        "\n",
        "        logits = self.dense(output)\n",
        "\n",
        "        return logits, state\n",
        "       \n",
        "    def zero_state(self, batch_size):\n",
        "        return (torch.zeros(flags.n_lay, batch_size, self.lstm_size),\n",
        "                torch.zeros(flags.n_lay, batch_size, self.lstm_size))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxcklPI-kB19",
        "colab_type": "code",
        "outputId": "124730bc-cb33-40bc-d4a5-4cc2ce9f5795",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "net = RNNModule(n_vocab, flags.seq_size,\n",
        "                    flags.embedding_size, flags.lstm_size)\n",
        "\n",
        "net.load_state_dict(torch.hub.load_state_dict_from_url('https://raw.githubusercontent.com/LauJohansson/DeepLearning_NLP_Friends/master/Data/net_final_online_ToGenerator_v1.pth'))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "net = net.to(device)\n",
        "\n",
        "net.eval()\n",
        "\n"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RNNModule(\n",
              "  (embedding): Embedding(11431, 256)\n",
              "  (lstm): LSTM(256, 512, num_layers=2, batch_first=True)\n",
              "  (dense): Linear(in_features=512, out_features=11431, bias=True)\n",
              "  (drop_out): Dropout(p=0.5, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5ucakDfkAJu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(device, net, words, n_vocab, vocab_to_int, int_to_vocab, top_k,manu_length):\n",
        "    #net.eval()\n",
        "\n",
        "    state_h, state_c = net.zero_state(1)\n",
        "    state_h = state_h.to(device)\n",
        "    state_c = state_c.to(device)\n",
        "    for w in words:\n",
        "        ix = torch.tensor([[vocab_to_int[w]]]).to(device)\n",
        "        output, (state_h, state_c) = net(ix, (state_h, state_c))\n",
        "    \n",
        "\n",
        "    _, top_ix = torch.topk(output[0], k=top_k)\n",
        "    choices = top_ix.tolist()\n",
        "    choice = np.random.choice(choices[0]) #A way to avoid always choose like \"and\" \"then\"..... \n",
        "\n",
        "    words.append(int_to_vocab[choice])\n",
        "\n",
        "    for _ in range(manu_length):\n",
        "        ix = torch.tensor([[choice]]).to(device)\n",
        "        output, (state_h, state_c) = net(ix, (state_h, state_c))\n",
        "\n",
        "        _, top_ix = torch.topk(output[0], k=top_k)\n",
        "        choices = top_ix.tolist()\n",
        "        choice = np.random.choice(choices[0])\n",
        "        words.append(int_to_vocab[choice])\n",
        "\n",
        "    \n",
        "\n",
        "    output= ' '.join(words)#.replace('. ', '.\\n')\n",
        "\n",
        "    output=output.replace('chandler:', '\\nchandler:')\n",
        "    output=output.replace('ross:', '\\nross:')\n",
        "    output=output.replace('joey:', '\\njoey')\n",
        "    output=output.replace('monica', '\\nmonica:')\n",
        "    output=output.replace('phoebe:', '\\nphoebe:')\n",
        "    output=output.replace('rachel:', '\\nrachel:')\n",
        "\n",
        "    \n",
        "    return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SU6yJiC9xCEe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_init_words():\n",
        "  initial_words = input(\"Please enter the first word of your Friends manuscript (seperate with space): \")\n",
        "  inital_words=initial_words.lower()\n",
        "  initial_words=initial_words.split()\n",
        "  initial_words=['[','scene:'] + initial_words\n",
        "  return initial_words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f921DXqP10Ix",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_manuscript():\n",
        "  initial_words=get_init_words()\n",
        "\n",
        "  number_of_words = int(input(\"Please enter the number of words that your Friends manuscript should contain: \"))\n",
        "\n",
        "  randomness = int(input(\"Choose a number from 1-100 (recommend 5) (if 1, then there is no randomness in word prediction): \"))\n",
        "\n",
        "  if randomness <1:\n",
        "    randomness=1\n",
        "  if randomness>100:\n",
        "    randomness=100  \n",
        "\n",
        "\n",
        "  #Only print of max 10000 words\n",
        "  if number_of_words>10000:\n",
        "    number_of_words=10000\n",
        "\n",
        "  #Make loop which checks if the entered words are in the vocab\n",
        "  first_loop=1\n",
        "  good_list=0\n",
        "\n",
        "  while good_list==0 or first_loop==1:\n",
        "    first_loop=0\n",
        "    good_list=1\n",
        "    result=all(elem in sorted_vocab for elem in  initial_words)\n",
        "\n",
        "    if not result:\n",
        "      good_list=0\n",
        "    if good_list==0:\n",
        "      print('Some of the words you entered is not a word that is used in Friends')\n",
        "      initial_words=get_init_words()\n",
        "  print('\\n')\n",
        "  print( predict(device, net, initial_words, n_vocab,\n",
        "                            vocab_to_int, int_to_vocab, randomness,number_of_words))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4EXdABCiX0Rm",
        "colab_type": "text"
      },
      "source": [
        "#Generate manuscript:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hsw82MiX2DTp",
        "colab_type": "code",
        "outputId": "561eeba7-0ca8-4fff-b37a-ecea32727f18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 938
        }
      },
      "source": [
        "generate_manuscript()"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Please enter the first word of your Friends manuscript (seperate with space): chandler and monica\n",
            "Please enter the number of words that your Friends manuscript should contain: 1000\n",
            "Choose a number from 1-100 (recommend 5) (if 1, then there is no randomness in word prediction): 5\n",
            "\n",
            "\n",
            "[ scene: chandler and \n",
            "monica: and joey's erm - <unk> and the gang , chandler enters from her . chandler , and rachel , chandler . and phoebe , joey , \n",
            "monica: . \n",
            "monica: . \n",
            "monica: ] \n",
            "joey what do it mean you ? \n",
            "chandler: oh , it wasn't a little bit of my stuff ! ( they kiss to see him and ross is shocked . ) \n",
            "ross: what are i talking to ? \n",
            "joey oh no ! you guys are not ! \n",
            "monica:: oh no . \n",
            "joey no - no i - i'm gonna go ! ( to joey and joey , joey and rachel both enter . ) \n",
            "joey hey ! ( she turns off to chandler and joey is sitting at him ) \n",
            "joey what ? what is this doing with you ? \n",
            "joey i know , but i'm not . ( chandler and ross both are stunned to leave . ) \n",
            "joey okay . \n",
            "chandler: okay - hey . \n",
            "chandler: okay ! \n",
            "chandler: so , i don't think i could get the part , i'm just not a big thing ! i - i know i am gonna go . \n",
            "monica:: oh my ! \n",
            "joey i can't . i know , i'm gonna go get my new <unk> . ( joey is shocked , then chandler gets a picture on it and chandler goes over with him and sees him on a date and the door . joey is shocked and chandler , who are in his mouth , but he is still not a <unk> . he starts looking in the window . he has just been in a <unk> , but she stops and starts looking around for his face and he starts a little more <unk> , which he does it again with her , but then then he goes over and sits back into his head . ) \n",
            "chandler: i can't ! \n",
            "chandler: what did you want to say that you were ? \n",
            "joey well it's not like i don't care . \n",
            "monica:: what did you say you mean i - i'm - it's a <unk> ? ! \n",
            "joey well it's just , i'm gonna get it . i'm - you're gonna have this stuff to go . \n",
            "joey ( on his hand ) okay - bye ! okay ! i have to take it out and i'm sorry . i don't have anything . i'm - you're - you're gonna do this , but you know , but i'm gonna get out and then you have to get a big deal with me , y'know ? \n",
            "chandler: okay , i'm just sure i could get the <unk> <unk> and then you get the money . ( they all leave . ] \n",
            "chandler: ( to ross ) you know , i was gonna be in your apartment with my <unk> - tay . \n",
            "joey ( entering ) hello . \n",
            "rachel: hi ! \n",
            "chandler: so i got the phone . \n",
            "joey i didn't think i could . i mean i was thinking about the <unk> i was just thinking . i think it just is so good to you ! \n",
            "chandler: i think i could . \n",
            "monica:: okay i can ! \n",
            "chandler: well , you don't want it . \n",
            "joey no , no ! it's a good deal ! y'know i just have the same part of you . and you have the first time ! ( the camera starts to walk out with a <unk> and <unk> a woman and chandler enters with a huge - old guy and a man ! the <unk> , and \n",
            "monica: and ross look in a couple and a little . \" and a woman <unk> , <unk> <unk> , <unk> the rest <unk> , which time they start . ) \n",
            "monica:: you guys have no idea , but i don't care . \n",
            "joey well what , if you have any idea . \n",
            "joey ( entering , wearing an idea of the fridge and the gang in front with his hands and his hands ) what did it say ? \n",
            "joey i - i'm just trying for my <unk> . \n",
            "joey ( to the duck , who is not a little <unk> . ) okay . so uh , you are not gonna have the same way that you don't know ? \n",
            "chandler: yeah , yeah ! ( pause , he turns around , then he goes into the kitchen , and sees that chandler is in his mouth with his face on a bonnet with his mouth and he puts a <unk> around in her head and his face and starts looking for his face as joey . joey is shocked . he doesn't like the rest . he doesn't like to be a very good guy and joey has just to see it on the table with him and the next time is the one with her head . it doesn't like the <unk> , it just like a <unk> - it doesn't sound of the one . ) ( the next flashback to joey . ] \n",
            "ross: what the hell ? ! what happened ! ( he looks at the ring . the camera pans off to joey and rachel who has to be a couple of their pants on a face . he starts laughing , and joey quickly glares around him , but she goes back into a while he sees that he has the same time , then the one that the first flashback is from his hand in the <unk> and chandler and joey start laughing . ] \n",
            "ross: i think you should have one more of that . ( the flashback walks over . ] \n",
            "rachel: okay . \n",
            "chandler: oh , it's okay . it's just the <unk> of the world , it's a big one for my new . ( joey enters , sees the guy in a sexy , and starts laughing with him ) \n",
            "monica:: i know , you know\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}