{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Friends_Generator.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LauJohansson/DeepLearning_NLP_Friends/blob/master/Friends_Generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2rMr8O2cFut",
        "colab_type": "text"
      },
      "source": [
        "#How to use"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5IyCeet2cKEX",
        "colab_type": "text"
      },
      "source": [
        "**Press \"runtime\" -> \"run all\"**\n",
        "\n",
        "**In the bottom of this notebook, there will pop up input-boxes. Here's an example of what you can give as inputs:**\n",
        "\n",
        "\n",
        "*Please enter the first word of your Friends manuscript (seperate with space): chandler and ross*\n",
        "\n",
        "*Please enter the number of words that your Friends manuscript should contain: 1000*\n",
        "\n",
        "*Choose a number from 1-100 (if 1, then there is no randomness in word prediction): 5*\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQs3Zti_Xq4A",
        "colab_type": "text"
      },
      "source": [
        "#Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtBQfWMUXw1k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from argparse import Namespace\n",
        "\n",
        "flags = Namespace(\n",
        "    mode = 'target', #'pretraining' is pretraining and 'target' is fine-tuning of either PTB data or friends data. \n",
        "    name='Friends',\n",
        "    seq_size= 32,                        #Sequence Length\n",
        "    batch_size=40,                       #Batch size\n",
        "    embedding_size=256,                  #Embedding size\n",
        "    lstm_size=512,                       #Hidden nodes size\n",
        "    gradients_norm=0.5,   \n",
        "    #initial_words=['banknote', 'berlitz'],\n",
        "    initial_words_train = [],\n",
        "    initial_words_valid= [],\n",
        "    predict_top_k=5,                    #Choose the k best next_word_prediction, and a random is chosing.\n",
        "    checkpoint_path='checkpoint',\n",
        "    total_epochs=100,                   #Choose number of epochs in training\n",
        "    learning_rate=0.001,                #Choose number of epochs in training\n",
        "    predict_every=1000,\n",
        "    #validation_corpus_size=len(valid_file.split()),\n",
        "    dropconnect_rate=0.4,               #Choose drop connect rate\n",
        "    n_lay=2,                            #Choose number of LSTM layers\n",
        "\n",
        "    #Set variational sequence length on/off\n",
        "    var_seq='Y',                            #Choose if variational sequence length is on/off\n",
        "    var_seq_std=2,                          #Choose std. dev. for norm distribution for var. seq. length ( in moment 1/2 of seq length)\n",
        "\n",
        "\n",
        "    #scheduler parameters\n",
        "    schedule_on='N',                        #Choose if LR-scheduler is on/off\n",
        "    triangular='N',                         #Choose 'Y' to turn on the slanted triangular LR. \n",
        "    cut_fracI=0.2,                          #Choose the fraction of iterations we increase the LR in STL\n",
        "    ratioI=32,                              #Choose how much smaller the lowest LR is from the maximum LR Î·max\n",
        "    nmaxI=0.0,                              #this will be set = learning_rate \n",
        "\n",
        "    #Use same drop-mask for drop-connect\n",
        "    same_drop_lstm='N',                     #Choose 'Y' if drop-connect all should use same mask\n",
        "    \n",
        "    #Dropout on embedding layer\n",
        "    drop_embed=0.5,                         #Choose dropoutrate for embedding dropout        \n",
        "\n",
        "    #Optimizer selection\n",
        "    optim_select='AdamW'                      #Choose between \"AdamW, SGD, ASGD\"\n",
        "\n",
        ")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IhaW_SmSzWgj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_data_from_file(train_file, batch_size, seq_size):\n",
        "  \n",
        "    text=train_file.split()\n",
        "    # Extend words_notinpretraining to text to get them as a part of the mapping dictionary\n",
        "    #text.extend(words_notinpretraining)\n",
        "    word_counts = Counter(text)\n",
        "    sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
        "    int_to_vocab = {k: w for k, w in enumerate(sorted_vocab)}\n",
        "    vocab_to_int = {w: k for k, w in int_to_vocab.items()}\n",
        "    n_vocab = len(int_to_vocab)\n",
        "\n",
        "    text=train_file.split()\n",
        "\n",
        "    print('Vocabulary size', n_vocab)\n",
        "\n",
        "    int_text = [vocab_to_int[w] for w in text]\n",
        "    num_batches = int(len(int_text) / (seq_size * batch_size))\n",
        "    in_text = int_text[:num_batches * batch_size * seq_size]\n",
        "    out_text = np.zeros_like(in_text)\n",
        "    out_text[:-1] = in_text[1:]\n",
        "    out_text[-1] = in_text[0]\n",
        "    in_text = np.reshape(in_text, (batch_size, -1))\n",
        "    out_text = np.reshape(out_text, (batch_size, -1))\n",
        "    return int_to_vocab, vocab_to_int, n_vocab, in_text, out_text, sorted_vocab"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Agm7zSvWzYoX",
        "colab_type": "code",
        "outputId": "6c4cef28-8679-4ee4-b35b-9a2c6434c649",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from urllib.request import urlopen\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "data = str(urlopen('https://raw.githubusercontent.com/LauJohansson/DeepLearning_NLP_Friends/master/Data/friends_train.txt').read(),encoding=\"utf-8\")\n",
        "\n",
        "int_to_vocab, vocab_to_int, n_vocab, in_text, out_text,sorted_vocab = get_data_from_file( \n",
        "          data, flags.batch_size, flags.seq_size)\n",
        "      \n",
        "\n"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary size 11431\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXoJ0zOClMTe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RNNModule(nn.Module):\n",
        "    def __init__(self, n_vocab, seq_size, embedding_size, lstm_size):\n",
        "        super(RNNModule, self).__init__()\n",
        "        self.seq_size = seq_size\n",
        "        self.lstm_size = lstm_size\n",
        "        self.embedding = nn.Embedding(n_vocab, embedding_size)\n",
        "        self.lstm=nn.LSTM(embedding_size,\n",
        "                            lstm_size,\n",
        "                            batch_first=True,num_layers=flags.n_lay)\n",
        "        \n",
        "        self.dense = nn.Linear(lstm_size, n_vocab)\n",
        "        self.drop_out=nn.Dropout(flags.drop_embed)\n",
        "        self.OneMaskOnly=torch.autograd.Variable(torch.Tensor(getattr(self.lstm,self.lstm._all_weights[0][0]).shape[0],\n",
        "                                                        getattr(self.lstm,self.lstm._all_weights[0][0]).shape[1]).uniform_().to(\"cuda\") > flags.dropconnect_rate)\n",
        "\n",
        "\n",
        "    def forward(self, x, prev_state):\n",
        "        embed = self.embedding(x)\n",
        "        embed=self.drop_out(embed)\n",
        "        orig=[]\n",
        "\n",
        "        #Make dropconnect\n",
        "        if self.training:\n",
        "          for i in range( len(self.lstm._all_weights[0])):\n",
        "            name = self.lstm._all_weights[0][i]\n",
        "            if name.find('LSTM.weight_hh_l')!=-1:      \n",
        "              orig.append(getattr(self.lstm,name))\n",
        "            \n",
        "              if flags.same_drop_lstm=='Y':\n",
        "                mask=self.OneMaskOnly\n",
        "              else:\n",
        "                mask=torch.autograd.Variable(torch.Tensor(getattr(self.lstm,name).shape[0],\n",
        "                                                        getattr(self.lstm,name).shape[1]).uniform_().to(\"cuda\") > flags.flags.dropconnect_rate)\n",
        "              setattr(self.lstm,name,torch.nn.Parameter(torch.mul(getattr(self.lstm,name),mask)))\n",
        "              \n",
        "              self.lstm.flatten_parameters()\n",
        "             \n",
        "\n",
        "        #LSTM forward\n",
        "        output, state = self.lstm(embed, prev_state)\n",
        "\n",
        "        #Set hh-weight back to original\n",
        "        if self.training:\n",
        "          a=0\n",
        "          for i in range( len(self.lstm._all_weights[0])):\n",
        "            name = self.lstm._all_weights[0][i]\n",
        "            if name.find('LSTM.weight_hh_l')!=-1:\n",
        "              print(orig)\n",
        "              setattr(self.lstm,name,orig[a])\n",
        "              #self.lstm.weight_hh_l0=orig\n",
        "              self.lstm.flatten_parameters()\n",
        "              a=+1\n",
        "\n",
        "        logits = self.dense(output)\n",
        "\n",
        "        return logits, state\n",
        "       \n",
        "    def zero_state(self, batch_size):\n",
        "        return (torch.zeros(flags.n_lay, batch_size, self.lstm_size),\n",
        "                torch.zeros(flags.n_lay, batch_size, self.lstm_size))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxcklPI-kB19",
        "colab_type": "code",
        "outputId": "151e985a-c8f3-41a6-9708-189b14266b18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "net = RNNModule(n_vocab, flags.seq_size,\n",
        "                    flags.embedding_size, flags.lstm_size)\n",
        "\n",
        "net.load_state_dict(torch.hub.load_state_dict_from_url('https://raw.githubusercontent.com/LauJohansson/DeepLearning_NLP_Friends/master/Data/net_final_online_ToGenerator_v1.pth'))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "net = net.to(device)\n",
        "\n",
        "net.eval()\n",
        "\n"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RNNModule(\n",
              "  (embedding): Embedding(11431, 256)\n",
              "  (lstm): LSTM(256, 512, num_layers=2, batch_first=True)\n",
              "  (dense): Linear(in_features=512, out_features=11431, bias=True)\n",
              "  (drop_out): Dropout(p=0.5, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5ucakDfkAJu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(device, net, words, n_vocab, vocab_to_int, int_to_vocab, top_k,manu_length):\n",
        "    #net.eval()\n",
        "\n",
        "    state_h, state_c = net.zero_state(1)\n",
        "    state_h = state_h.to(device)\n",
        "    state_c = state_c.to(device)\n",
        "    for w in words:\n",
        "        ix = torch.tensor([[vocab_to_int[w]]]).to(device)\n",
        "        output, (state_h, state_c) = net(ix, (state_h, state_c))\n",
        "    \n",
        "\n",
        "    _, top_ix = torch.topk(output[0], k=top_k)\n",
        "    choices = top_ix.tolist()\n",
        "\n",
        "    #Removing \"<unk>\" from the predictions\n",
        "    for word in choices[0]:  # iterating on a copy since removing will mess things up\n",
        "      if word==vocab_to_int[\"<unk>\"]:\n",
        "        choices[0].remove(word)\n",
        "\n",
        "    choice = np.random.choice(choices[0]) #A way to avoid always choose like \"and\" \"then\"..... \n",
        "\n",
        "    words.append(int_to_vocab[choice])\n",
        "\n",
        "    for _ in range(manu_length):\n",
        "        ix = torch.tensor([[choice]]).to(device)\n",
        "        output, (state_h, state_c) = net(ix, (state_h, state_c))\n",
        "\n",
        "        _, top_ix = torch.topk(output[0], k=top_k)\n",
        "        choices = top_ix.tolist()\n",
        "        #Removing \"<unk>\" from the predictions\n",
        "        for word in choices[0]:  # iterating on a copy since removing will mess things up\n",
        "          if word==vocab_to_int[\"<unk>\"]:\n",
        "            choices[0].remove(word)\n",
        "         \n",
        "        choice = np.random.choice(choices[0])\n",
        "        words.append(int_to_vocab[choice])\n",
        "\n",
        "    \n",
        "\n",
        "    output= ' '.join(words)#.replace('. ', '.\\n')\n",
        "\n",
        "    output=output.replace('chandler:', '\\nchandler:')\n",
        "    output=output.replace('ross:', '\\nross:')\n",
        "    output=output.replace('joey:', '\\njoey')\n",
        "    output=output.replace('monica', '\\nmonica:')\n",
        "    output=output.replace('phoebe:', '\\nphoebe:')\n",
        "    output=output.replace('rachel:', '\\nrachel:')\n",
        "\n",
        "    \n",
        "    return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SU6yJiC9xCEe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_init_words():\n",
        "  initial_words = input(\"Please enter the first word of your Friends manuscript (seperate with space): \")\n",
        "  inital_words=initial_words.lower()\n",
        "  initial_words=initial_words.split()\n",
        "  initial_words=['[','scene:'] + initial_words\n",
        "  return initial_words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f921DXqP10Ix",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_manuscript():\n",
        "  initial_words=get_init_words()\n",
        "\n",
        "  number_of_words = int(input(\"Please enter the number of words that your Friends manuscript should contain: \"))\n",
        "\n",
        "  randomness = int(input(\"Choose a number from 1-100 (recommend 5) (if 1, then there is no randomness in word prediction): \"))\n",
        "\n",
        "  if randomness <1:\n",
        "    randomness=1\n",
        "  if randomness>100:\n",
        "    randomness=100  \n",
        "\n",
        "\n",
        "  #Only print of max 10000 words\n",
        "  if number_of_words>10000:\n",
        "    number_of_words=10000\n",
        "\n",
        "  #Make loop which checks if the entered words are in the vocab\n",
        "  first_loop=1\n",
        "  good_list=0\n",
        "\n",
        "  while good_list==0 or first_loop==1:\n",
        "    first_loop=0\n",
        "    good_list=1\n",
        "    result=all(elem in sorted_vocab for elem in  initial_words)\n",
        "\n",
        "    if not result:\n",
        "      good_list=0\n",
        "    if good_list==0:\n",
        "      print('Some of the words you entered is not a word that is used in Friends')\n",
        "      initial_words=get_init_words()\n",
        "  print('\\n')\n",
        "  print( predict(device, net, initial_words, n_vocab,\n",
        "                            vocab_to_int, int_to_vocab, randomness,number_of_words))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4EXdABCiX0Rm",
        "colab_type": "text"
      },
      "source": [
        "#Generate manuscript:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hsw82MiX2DTp",
        "colab_type": "code",
        "outputId": "8a3a3ae3-5d3d-4a92-fd1f-097292c05f33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 921
        }
      },
      "source": [
        "generate_manuscript()"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Please enter the first word of your Friends manuscript (seperate with space): chandler and monica\n",
            "Please enter the number of words that your Friends manuscript should contain: 1000\n",
            "Choose a number from 1-100 (recommend 5) (if 1, then there is no randomness in word prediction): 5\n",
            "\n",
            "\n",
            "[ scene: chandler and \n",
            "monica: and chandler , \n",
            "monica: are talking . joey has to get the picture on it with a little . . he walks out with her bedroom in the hallway and starts wearing some of her face to get his hands , and he is there for his face , he turns and starts laughing at her and he sees his head . ) \n",
            "chandler: hey , i think i should have this part for the time ! i mean , it's just , this is a good deal , y'know i think it's really good to be like you , i just don't like that ! \n",
            "joey oh ! you are not going for a little bit for your friends ? \n",
            "monica:: no i don't want . i am sorry . \n",
            "joey oh yeah . \n",
            "monica:: oh my goodness . i mean you have the first day . \n",
            "joey i can't ! i am sorry . \n",
            "chandler: ( entering with her room , with his mouthful of a guy and is a little - woman - - old , and - a lot about to say it - that's not like you want me for . \n",
            "ross: well i just don't care what to tell you , but it's okay . y'know , you have no one , i don't have anything about this , and i'm sorry , and i just have been to go . ( she starts over his apartment . chandler enters ) ( he starts laughing and chandler enters with her , he is looking in the bathroom ) \n",
            "chandler: ( entering from joey and \n",
            "monica: , joey . ) what did i have you been here ! ( joey is wearing a little - tay , and the one in his hand , which one of it is . \" the one from the duck and joey walks out with him in the living room ) \n",
            "ross: you guys are going on ? \n",
            "monica:: i know . ( she starts over to find her . she goes back in a mirror ) ( he turns around to find the one of his face on the door ) i mean you don't get a good time with it and then it doesn't have the one . ( she starts the phone , the duck rings , chandler , joey . chandler , chandler and \n",
            "monica: both look out the window , but ross and \n",
            "monica: start laughing . ) \n",
            "chandler: i thought we were going out . i mean we have a lot . \n",
            "chandler: yeah ! \n",
            "joey ( looking down in his ear in the background . joey ) i think it's the one where you said . \n",
            "monica:: well you don't have a problem ! \n",
            "chandler: ( pause , but she turns out the phone to reveal joey , joey is looking for the tv to find a huge face ) ( chandler walks out to chandler . ] \n",
            "joey ( in the living ) what are the guys are talking at me ? ! \n",
            "joey what ? ! ! \n",
            "chandler: i know ! ( to the gang ) oh , you don't know that , you can't have a problem with me ? i am ! ( pause ) what do i mean do that , do you think you're gonna tell him what to tell me you ? ( she looks out on his hand as a woman , he turns and glares in a face ) i don't want to know , i'm so excited about it for my life , and it's a little bit of you and you have to go out , and i - i'll - i - i'll just tell you about it and you know that i would just have the most good day for my friends ! i know ! i am so glad you can have a lot of money for me and the best . \n",
            "monica:: i am so sorry i do ! ( joey enters and walks into the bedroom , with his head . ) \n",
            "joey oh ! ! \n",
            "joey i can't believe you did it ! \n",
            "chandler: well it's the way to see it . \n",
            "joey well , it was so weird . i mean , it's a little like to be like , y'know what you said ? \n",
            "chandler: ( to phoebe ) you have the best guy , that is the most amazing thing ! i don't think you want to go over to him , i just , but i'm not gonna have to do this to her ! ( joey is so excited , chandler is laughing ) \n",
            "chandler: oh my , god . i don't think you want a lot , i'm not even a big one of my life ! \n",
            "monica:: you know ! \n",
            "chandler: i don't think i don't want that ! \n",
            "monica:: what , i think you don't think you don't know ? \n",
            "chandler: well i - i'm sorry about it . \n",
            "monica:: oh ! \n",
            "joey hey ! ( he kisses him ) \n",
            "chandler: oh yeah ! \n",
            "chandler: i think you know , i just had a problem . \n",
            "joey oh , i can't believe i don't want that you have . \n",
            "joey oh yeah ? yeah , yeah , but it was so much . \n",
            "joey oh no ! no . i mean it doesn't mean anything to me ! i - i'm - i'm just a good - year person ! ( pause . and joey turns around ) i mean you don't know , you are not really gonna be here with me , and i - it's like a big mistake and i think you were going out of it , but it's like i was just gonna do this , y'know . and if i'm a lot better , y'know if you want me . \n",
            "chandler: okay ? \n",
            "chandler: okay . \n",
            "joey okay ! \n",
            "joey hey\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}