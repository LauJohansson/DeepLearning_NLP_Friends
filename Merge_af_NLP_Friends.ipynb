{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Merge af NLP Friends.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "YiIMKIBRmeI_",
        "CYSUF18ZN6cf",
        "kf20oH-l9pIZ",
        "Gb22lHosOTjd",
        "uy8anIthSTXZ",
        "g-OSrXMTeA6i"
      ],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LauJohansson/DeepLearning_NLP_Friends/blob/master/Merge_af_NLP_Friends.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YiIMKIBRmeI_",
        "colab_type": "text"
      },
      "source": [
        "#**Introduction**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twGMoGa7mhCd",
        "colab_type": "text"
      },
      "source": [
        "We build a Recurrent Neural Network (RNN) using the Long Short-Term Memory (LSTM) architecture. Through tuning of hyperparameters and regularization through variational sequence length and DropConnect we achieve a perplexity value on the Penn Treebank dataset. We then utilize the found model to generate dialogue for the TV-series “Friends”. To this end, we pretrain the model on Wikipedia text and further tune the hyperparameters to fit “Friends”. Finally, we show a subset of the predicted dialogue and visualise the found semantics of “Friends” through t-SNE on the embedding weights.  \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LtIKDkJSg2XV",
        "colab_type": "text"
      },
      "source": [
        "NB: When opening from Github you can not load or save pretrained model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhzDijSPm9no",
        "colab_type": "text"
      },
      "source": [
        "##Acknowledgements"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wX9wD9wbnJSY",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "This notebook is inspired by the following NLP Tutorial:https://github.com/graykode/nlp-tutorial\n",
        "Made by Tae-Hwan Jung. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DK8RFoquSj5",
        "colab_type": "text"
      },
      "source": [
        "# **Initialising Friends data**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLpYqib8PeGH",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://drive.google.com/uc?id=1JzPdBavHFL-8R1Tc0F6z9KlELR326MXy)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84ed6CSdPlRY",
        "colab_type": "code",
        "outputId": "27d96ff2-75eb-4717-8417-00d23fa3da26",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import collections\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import glob\n",
        "import io\n",
        "import matplotlib. pyplot as plt\n",
        "from collections import Counter\n",
        "# Importing drive method from colab for accessing google drive\n",
        "from google.colab import drive\n",
        "\n",
        "# Mounting drive\n",
        "# This will require authentication : Follow the steps as guided\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "try: #If files in google drive\n",
        "  datafrom='Drive'\n",
        "  friends_train = open(\"/content/drive/My Drive/DL_project/friends_train.txt\").read()\n",
        "  friends_valid = open(\"/content/drive/My Drive/DL_project/friends_valid.txt\").read()\n",
        "  friends_test = open(\"/content/drive/My Drive/DL_project/friends_test.txt\").read()\n",
        "\n",
        "  pretraining_train = open(\"/content/drive/My Drive/DL_project/wikitrainclean.txt\").read()\n",
        "  pretraining_valid = open(\"/content/drive/My Drive/DL_project/wikivalidclean.txt\").read()\n",
        "  pretraining_test = open(\"/content/drive/My Drive/DL_project/wikitestclean.txt\").read()\n",
        "\n",
        "  \n",
        "except: #For github version\n",
        "  datafrom='Github'\n",
        "  from urllib.request import urlopen\n",
        "\n",
        "  friends_train=str(urlopen('https://raw.githubusercontent.com/LauJohansson/DeepLearning_NLP_Friends/master/Data/friends_train.txt').read(),encoding=\"utf-8\")\n",
        "  friends_valid = str(urlopen('https://raw.githubusercontent.com/LauJohansson/DeepLearning_NLP_Friends/master/Data/friends_valid.txt').read(),encoding=\"utf-8\")\n",
        "  friends_test = str(urlopen('https://raw.githubusercontent.com/LauJohansson/DeepLearning_NLP_Friends/master/Data/friends_test.txt').read(),encoding=\"utf-8\")\n",
        "\n",
        "  pretraining_train = str(urlopen('https://raw.githubusercontent.com/LauJohansson/DeepLearning_NLP_Friends/master/Data/seinfeldtrainclean.txt').read(),encoding=\"utf-8\")\n",
        "  pretraining_valid = str(urlopen('https://raw.githubusercontent.com/LauJohansson/DeepLearning_NLP_Friends/master/Data/seinfeldvalidclean.txt').read(),encoding=\"utf-8\")\n",
        "  pretraining_test = str(urlopen('https://raw.githubusercontent.com/LauJohansson/DeepLearning_NLP_Friends/master/Data/seinfeldtestclean.txt').read(),encoding=\"utf-8\")\n",
        "\n",
        "  prenet=io.BytesIO(urlopen('https://drive.google.com/file/d/1HsyK1m9Xp0VfOuGsuMwWKvo_cvyyWCKD/view?usp=sharing').read())"
      ],
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIQl-Df3TiEZ",
        "colab_type": "code",
        "outputId": "46db7857-054f-473d-bb06-1e86ba78f734",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "# check data\n",
        "print(\"Traindata snippet friends:    \" + friends_train[:100])\n",
        "print(\"Validdata snippet friends:    \" + friends_valid[:100])\n",
        "print(\"Testdata snippet friends:     \" + friends_test[:100])\n",
        "print()\n",
        "print(\"Traindata snippet pretraining:       \" + pretraining_train[:100])\n",
        "print(\"Validdata snippet pretraining:       \" + pretraining_valid[:100])\n",
        "print(\"Testdata snippet pretraining:        \" + pretraining_test[:100])"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Traindata snippet friends:    [ scene: central perk , chandler , joey , phoebe , and monica are there . ] monica: there's nothing \n",
            "Validdata snippet friends:    parts that i really wanted . you always believed in me man . even , even when i didn't believe in my\n",
            "Testdata snippet friends:     is at the half - opened door ] phoebe: ( in a strange heavy accent ) hello <unk> , it's time for you\n",
            "\n",
            "Traindata snippet pretraining:       int . comedy club \\u2013 night ( jerry is on stage , performing . ) jerry: do you know what this is \n",
            "Validdata snippet pretraining:       : sounds like a nice girl . hey jerry , is it all right if i put some stuff in your fridge ? 'cause \n",
            "Testdata snippet pretraining:        stack of mail out at the desk that belongs to you . now , you want that mail , don't you mr . kramer\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvq6sH9HSqtx",
        "colab_type": "code",
        "outputId": "ee154c3d-f6c2-4521-f607-debd64bc6070",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Find words that occur in friends data, but not in pretraining data\n",
        "words_notinpretraining = list(set(friends_train.split()).difference(pretraining_train.split()))\n",
        "words_notinpretraining[0:5]"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['bulge', 'liver', 'billion', 'aspirin', 'father;']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBJq1zyuuGkz",
        "colab_type": "text"
      },
      "source": [
        "# **Initialising Penn Tree Bank data**\n",
        "![alt text](https://drive.google.com/uc?id=1A8SY9dmsLx2hoanWWWhT_IJnJcJOmluR)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAp6BEbqQEDe",
        "colab_type": "text"
      },
      "source": [
        "The following site is used as inspiration to get PTB data: https://corochann.com/penn-tree-bank-ptb-dataset-introduction-1456.html\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zoW2qb_YQMil",
        "colab_type": "code",
        "outputId": "e4eafbb4-07c0-450e-f8b6-3c665f5f8b35",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "'''\n",
        "#Get PTB data\n",
        "from __future__ import print_function\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        " \n",
        " \n",
        "import numpy as np\n",
        " \n",
        "import chainer\n",
        "train, val, test = chainer.datasets.get_ptb_words()\n",
        "\n",
        "ptb_dict = chainer.datasets.get_ptb_words_vocabulary()\n",
        "print('Number of vocabulary', len(ptb_dict))\n",
        "print('ptb_dict', ptb_dict)\n",
        "\n",
        "ptb_word_id_dict = ptb_dict\n",
        "ptb_id_word_dict = dict((v,k) for k,v in ptb_word_id_dict.items())\n",
        "\n",
        "\n",
        "#Create data as str\n",
        "dataPTB=' '.join([ptb_id_word_dict[i] for i in train[:]])\n",
        "dataPTBtest=' '.join([ptb_id_word_dict[i] for i in test[:]])\n",
        "dataPTBvalid=' '.join([ptb_id_word_dict[i] for i in val[:]])\n",
        "'''"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n#Get PTB data\\nfrom __future__ import print_function\\nimport os\\nimport matplotlib.pyplot as plt\\n%matplotlib inline\\n \\n \\nimport numpy as np\\n \\nimport chainer\\ntrain, val, test = chainer.datasets.get_ptb_words()\\n\\nptb_dict = chainer.datasets.get_ptb_words_vocabulary()\\nprint('Number of vocabulary', len(ptb_dict))\\nprint('ptb_dict', ptb_dict)\\n\\nptb_word_id_dict = ptb_dict\\nptb_id_word_dict = dict((v,k) for k,v in ptb_word_id_dict.items())\\n\\n\\n#Create data as str\\ndataPTB=' '.join([ptb_id_word_dict[i] for i in train[:]])\\ndataPTBtest=' '.join([ptb_id_word_dict[i] for i in test[:]])\\ndataPTBvalid=' '.join([ptb_id_word_dict[i] for i in val[:]])\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIVP8LXaNyGA",
        "colab_type": "text"
      },
      "source": [
        "#***Import and initializing hyperparameters***\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2otEIPN5TPc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import os\n",
        "from argparse import Namespace\n",
        "\n",
        "flags = Namespace(\n",
        "    mode = 'friends', #'pretraining' is pretraining and 'friends' is fine-tuning.\n",
        "    name='Friends',\n",
        "    train_file_pretraining = pretraining_train,                    #shift between data (data or dataPTB)\n",
        "    valid_file_pretraining = pretraining_valid, \n",
        "    test_file_pretraining = pretraining_test,   \n",
        "    train_file_friends= friends_train,                    #shift between data (data or dataPTB)\n",
        "    valid_file_friends= friends_valid, \n",
        "    test_file_friends = friends_test,\n",
        "    seq_size= 32, #16,\n",
        "    batch_size=40, #16,\n",
        "    embedding_size=256, #300,\n",
        "    lstm_size=512, #,300,                      #Hidden size\n",
        "    gradients_norm=0.5,\n",
        "    #initial_words=['banknote', 'berlitz'],\n",
        "    initial_words_train = [],\n",
        "    initial_words_valid= [],\n",
        "    predict_top_k=5,                    #Choose the k best next_word_prediction, and a random is chosing.\n",
        "    checkpoint_path='checkpoint',\n",
        "    total_epochs=100,   #30                  #Change number of epochs in training\n",
        "    learning_rate=0.001,\n",
        "    predict_every=1000,\n",
        "    #validation_corpus_size=len(valid_file.split()),\n",
        "    dropconnect_rate=0.4,\n",
        "    n_lay=2,\n",
        "\n",
        "    #Set variational sequence length on/off\n",
        "    var_seq='Y',                            #Choose if variational sequence length is on/off\n",
        "    var_seq_std=2,                          #Choose std. dev. for norm distribution for var. seq. length ( in moment 1/2 of seq length)\n",
        "\n",
        "\n",
        "    #scheduler parameters\n",
        "    schedule_on='N',                        #Choose if LR-scheduler is on/off\n",
        "    triangular='N',                         #Choose 'Y' to turn on the slanted triangular LR. \n",
        "    cut_fracI=0.2,                          #Choose the fraction of iterations we increase the LR in STL\n",
        "    ratioI=32,                              #Choose how much smaller the lowest LR is from the maximum LR ηmax\n",
        "    nmaxI=0.0,                              #this will be set = learning_rate \n",
        "\n",
        "    #Use same drop-mask for drop-connect\n",
        "    same_drop_lstm='N',                     #Choose 'Y' if drop-connect all should use same mask\n",
        "    \n",
        "    #Dropout on embedding layer\n",
        "    drop_embed=0.5,                         #Choose dropoutrate for embedding dropout        \n",
        "\n",
        "    #Optimizer selection\n",
        "    optim_select='AdamW'                      #Choose between \"AdamW, SGD, ASGD\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        ")\n",
        "\n",
        "\n",
        "#initialising two random words, the train and test data should use\n",
        "if flags.mode == 'friends':\n",
        "  for i in range(0,1):\n",
        "    flags.initial_words_train.append(np.random.choice(flags.train_file_friends.split()))\n",
        "    flags.initial_words_valid.append(np.random.choice(flags.valid_file_friends.split()))\n",
        "    flags.validation_corpus_size = len(flags.valid_file_friends.split())\n",
        "    \n",
        "if flags.mode == 'pretraining':\n",
        "  for i in range(0,1):\n",
        "    flags.initial_words_train.append(np.random.choice(flags.train_file_pretraining.split()))\n",
        "    flags.initial_words_valid.append(np.random.choice(flags.valid_file_pretraining.split()))\n",
        "    flags.validation_corpus_size = len(flags.valid_file_pretraining.split())\n",
        "\n",
        "flags.nmaxI=flags.learning_rate"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKozVznDlxMS",
        "colab_type": "text"
      },
      "source": [
        "#**Define functions and set up model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYSUF18ZN6cf",
        "colab_type": "text"
      },
      "source": [
        "##Function to get data for main and batch-function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjlDy7cq5UA8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_data_from_file(train_file, batch_size, seq_size):\n",
        "  \n",
        "    text=train_file.split()\n",
        "    # Extend words_notinpretraining to text to get them as a part of the mapping dictionary\n",
        "    text.extend(words_notinpretraining)\n",
        "    word_counts = Counter(text)\n",
        "    sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
        "    int_to_vocab = {k: w for k, w in enumerate(sorted_vocab)}\n",
        "    vocab_to_int = {w: k for k, w in int_to_vocab.items()}\n",
        "    n_vocab = len(int_to_vocab)\n",
        "\n",
        "    text=train_file.split()\n",
        "\n",
        "    print('Vocabulary size', n_vocab)\n",
        "\n",
        "    int_text = [vocab_to_int[w] for w in text]\n",
        "    num_batches = int(len(int_text) / (seq_size * batch_size))\n",
        "    in_text = int_text[:num_batches * batch_size * seq_size]\n",
        "    out_text = np.zeros_like(in_text)\n",
        "    out_text[:-1] = in_text[1:]\n",
        "    out_text[-1] = in_text[0]\n",
        "    in_text = np.reshape(in_text, (batch_size, -1))\n",
        "    out_text = np.reshape(out_text, (batch_size, -1))\n",
        "    return int_to_vocab, vocab_to_int, n_vocab, in_text, out_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7pPsjVmq5Wes",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_batches(in_text, out_text, batch_size, seq_size):\n",
        "    num_batches = np.prod(in_text.shape) // (seq_size * batch_size)\n",
        "    for i in range(0, num_batches * seq_size, seq_size):\n",
        "        yield in_text[:, i:i+seq_size], out_text[:, i:i+seq_size]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VkuuPkykOH6k",
        "colab_type": "text"
      },
      "source": [
        "##Setting up the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncUCPVoc5bLd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RNNModule(nn.Module):\n",
        "    def __init__(self, n_vocab, seq_size, embedding_size, lstm_size):\n",
        "        super(RNNModule, self).__init__()\n",
        "        self.seq_size = seq_size\n",
        "        self.lstm_size = lstm_size\n",
        "        self.embedding = nn.Embedding(n_vocab, embedding_size)\n",
        "        self.lstm=nn.LSTM(embedding_size,\n",
        "                            lstm_size,\n",
        "                            batch_first=True,num_layers=flags.n_lay)\n",
        "        \n",
        "        self.dense = nn.Linear(lstm_size, n_vocab)\n",
        "        self.drop_out=nn.Dropout(flags.drop_embed)\n",
        "        self.OneMaskOnly=torch.autograd.Variable(torch.Tensor(getattr(self.lstm,self.lstm._all_weights[0][0]).shape[0],\n",
        "                                                        getattr(self.lstm,self.lstm._all_weights[0][0]).shape[1]).uniform_().to(\"cuda\") > flags.dropconnect_rate)\n",
        "\n",
        "\n",
        "    def forward(self, x, prev_state):\n",
        "        embed = self.embedding(x)\n",
        "        embed=self.drop_out(embed)\n",
        "        orig=[]\n",
        "\n",
        "        #Make dropconnect\n",
        "        if self.training:\n",
        "          for i in range( len(self.lstm._all_weights[0])):\n",
        "            name = self.lstm._all_weights[0][i]\n",
        "            if name.find('LSTM.weight_hh_l')!=-1:      \n",
        "              orig.append(getattr(self.lstm,name))\n",
        "            \n",
        "              if flags.same_drop_lstm=='Y':\n",
        "                mask=self.OneMaskOnly\n",
        "              else:\n",
        "                mask=torch.autograd.Variable(torch.Tensor(getattr(self.lstm,name).shape[0],\n",
        "                                                        getattr(self.lstm,name).shape[1]).uniform_().to(\"cuda\") > flags.flags.dropconnect_rate)\n",
        "              setattr(self.lstm,name,torch.nn.Parameter(torch.mul(getattr(self.lstm,name),mask)))\n",
        "              \n",
        "              self.lstm.flatten_parameters()\n",
        "             \n",
        "\n",
        "        #LSTM forward\n",
        "        output, state = self.lstm(embed, prev_state)\n",
        "\n",
        "        #Set hh-weight back to original\n",
        "        if self.training:\n",
        "          a=0\n",
        "          for i in range( len(self.lstm._all_weights[0])):\n",
        "            name = self.lstm._all_weights[0][i]\n",
        "            if name.find('LSTM.weight_hh_l')!=-1:\n",
        "              print(orig)\n",
        "              setattr(self.lstm,name,orig[a])\n",
        "              #self.lstm.weight_hh_l0=orig\n",
        "              self.lstm.flatten_parameters()\n",
        "              a=+1\n",
        "\n",
        "        logits = self.dense(output)\n",
        "\n",
        "        return logits, state\n",
        "       \n",
        "    def zero_state(self, batch_size):\n",
        "        return (torch.zeros(flags.n_lay, batch_size, self.lstm_size),\n",
        "                torch.zeros(flags.n_lay, batch_size, self.lstm_size))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kf20oH-l9pIZ",
        "colab_type": "text"
      },
      "source": [
        "##Defining ASGD\n",
        "from: https://pytorch.org/docs/stable/_modules/torch/optim/asgd.html\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63Qg4X449uXi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import torch\n",
        "#from .optimizer import Optimizer\n",
        "\n",
        "\n",
        "class ASGD(torch.optim.Optimizer):\n",
        "    \"\"\"Implements Averaged Stochastic Gradient Descent.\n",
        "\n",
        "    It has been proposed in `Acceleration of stochastic approximation by\n",
        "    averaging`_.\n",
        "\n",
        "    Arguments:\n",
        "        params (iterable): iterable of parameters to optimize or dicts defining\n",
        "            parameter groups\n",
        "        lr (float, optional): learning rate (default: 1e-2)\n",
        "        lambd (float, optional): decay term (default: 1e-4)\n",
        "        alpha (float, optional): power for eta update (default: 0.75)\n",
        "        t0 (float, optional): point at which to start averaging (default: 1e6)\n",
        "        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n",
        "\n",
        "    .. _Acceleration of stochastic approximation by averaging:\n",
        "        http://dl.acm.org/citation.cfm?id=131098\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, params, lr=1e-2, lambd=1e-4, alpha=0.75, t0=1e6, weight_decay=0):\n",
        "        if not 0.0 <= lr:\n",
        "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
        "        if not 0.0 <= weight_decay:\n",
        "            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n",
        "\n",
        "        defaults = dict(lr=lr, lambd=lambd, alpha=alpha, t0=t0,\n",
        "                        weight_decay=weight_decay)\n",
        "        super(ASGD, self).__init__(params, defaults)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        \"\"\"Performs a single optimization step.\n",
        "\n",
        "        Arguments:\n",
        "            closure (callable, optional): A closure that reevaluates the model\n",
        "                and returns the loss.\n",
        "        \"\"\"\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                grad = p.grad.data\n",
        "                if grad.is_sparse:\n",
        "                    raise RuntimeError('ASGD does not support sparse gradients')\n",
        "                state = self.state[p]\n",
        "\n",
        "                # State initialization\n",
        "                if len(state) == 0:\n",
        "                    state['step'] = 0\n",
        "                    state['eta'] = group['lr']\n",
        "                    state['mu'] = 1\n",
        "                    state['ax'] = torch.zeros_like(p.data)\n",
        "\n",
        "                state['step'] += 1\n",
        "\n",
        "                if group['weight_decay'] != 0:\n",
        "                    grad = grad.add(group['weight_decay'], p.data)\n",
        "\n",
        "                # decay term\n",
        "                p.data.mul_(1 - group['lambd'] * state['eta'])\n",
        "\n",
        "                # update parameter\n",
        "                p.data.add_(-state['eta'], grad)\n",
        "\n",
        "                # averaging\n",
        "                if state['mu'] != 1:\n",
        "                    state['ax'].add_(p.data.sub(state['ax']).mul(state['mu']))\n",
        "                else:\n",
        "                    state['ax'].copy_(p.data)\n",
        "\n",
        "                # update eta and mu\n",
        "                state['eta'] = (group['lr'] /\n",
        "                                math.pow((1 + group['lambd'] * group['lr'] * state['step']), group['alpha']))\n",
        "                state['mu'] = 1 / max(1, state['step'] - group['t0'])\n",
        "\n",
        "        return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gb22lHosOTjd",
        "colab_type": "text"
      },
      "source": [
        "##Define function to call optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZl2BaWK5mRx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_loss_and_train_op(net, lrI):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    if (flags.optim_select=='ASGD'):\n",
        "      optimizer = torch.optim.ASGD(net.parameters(), lr=lrI,weight_decay=0,t0=0,lambd=0) #decay, t0 and lambd taken from article\n",
        "    elif (flags.optim_select=='SGD'):\n",
        "      optimizer = torch.optim.SGD(net.parameters(), lr=lrI) #decay, t0 and lambd taken from article\n",
        "    else:  \n",
        "      optimizer = torch.optim.AdamW(net.parameters(), lr=lrI,weight_decay=0.3)\n",
        "  \n",
        "    return criterion, optimizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uy8anIthSTXZ",
        "colab_type": "text"
      },
      "source": [
        "##Define function to schedule optimizer for ASGD\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Lqz2wRTUsAw",
        "colab_type": "text"
      },
      "source": [
        "Article used: [Universal Language Model Fine-tuning for Text Classification](https://www.aclweb.org/anthology/P18-1031.pdf?fbclid=IwAR0-TADs3LWh74b4xbA2QW5OYM5-_5iFu2EBjd_0-KVWOUytnBV5TeS9KGo)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dC3LvPuXSTqm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#T=number of epochs * number of updates per epoch\n",
        "#\"cut_frac\" is the fraction of iterations we increase the LR\n",
        "# \"cut\" is the iteration when we switch from increasing to decreasing the LR\n",
        "# \"p\" is the fraction of the number of iterations we have increased or will decrease the LR respectively\n",
        "# \"ratio\" specifies how much smaller the lowest LR is from the maximum LR nmax\n",
        "#\"nt\"/return value is the learning rate at iteration t.\n",
        "\n",
        "#The paper generally use cut_frac = 0.1, ratio = 32 and ηmax = 0.01\n",
        "\n",
        "\n",
        "def triangular_lr_func(t,T,cut_frac=flags.cut_fracI, ratio=flags.ratioI, nmax=flags.nmaxI ):\n",
        "    cut=np.floor(T*cut_frac)\n",
        "\n",
        "    if (t<cut): p=t/cut\n",
        "    else: p=1-((t-cut)/(cut*(ratio-1)))\n",
        "\n",
        "\n",
        "    return nmax * (1+p*(ratio-1))/ratio"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-OSrXMTeA6i",
        "colab_type": "text"
      },
      "source": [
        "##Custommade scheduler\n",
        "This scheduler is an example on how ASGD LR can be decreased for each iteration. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9aq4WV5lLes_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def easy_schedule(t,nmax=flags.nmaxI):\n",
        "  result = nmax-(t*0.0001)\n",
        "\n",
        "  if(result<0.1): result=0.1\n",
        "\n",
        "  return result\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXgqNYjZOW9X",
        "colab_type": "text"
      },
      "source": [
        "#**Define main**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNF3L6ss5qHL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main():\n",
        "    torch.manual_seed(11)\n",
        "    print('------------------------------------------------------------------------  ')\n",
        "    print('----------------------Go into training mode--------------------------------  ')\n",
        "    print(\"----------------------using %s data------------------------\" % flags.mode)\n",
        "    print('------------------------------------------------------------------------  ')\n",
        "    \n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    # Pre-training mode on pretrainingtext\n",
        "    # Start network from scratch\n",
        "    if flags.mode == 'pretraining': \n",
        "      int_to_vocab, vocab_to_int, n_vocab, in_text, out_text = get_data_from_file(\n",
        "          flags.train_file_pretraining, flags.batch_size, flags.seq_size)\n",
        "           \n",
        "    # Fine-tuning mode on friends\n",
        "    if flags.mode == 'friends': \n",
        "\n",
        "      # Get int_to_vocab, vocab_to_int, n_vocab from pretraining text pre-training\n",
        "      int_to_vocab, vocab_to_int, n_vocab, _, _ = get_data_from_file( \n",
        "          flags.train_file_pretraining, flags.batch_size, flags.seq_size)\n",
        "      \n",
        "      # Load and store data for friends-training\n",
        "      text = flags.train_file_friends.split()\n",
        "      int_text = [vocab_to_int[v] for v in text]\n",
        "      var1 = len(int_text) % flags.batch_size \n",
        "      in_text =   int_text[:] if var1==0 else  int_text[:-var1]              ## Added -19 to make divisble with 20 batch size\n",
        "      out_text = np.zeros_like(in_text)\n",
        "      out_text[:-1] = in_text[1:]\n",
        "      out_text[-1] = in_text[0]\n",
        "      in_text = np.reshape(in_text, (flags.batch_size, -1))\n",
        "      out_text = np.reshape(out_text, (flags.batch_size, -1))\n",
        "\n",
        "\n",
        "    if flags.mode == 'pretraining': \n",
        "      # Load and store data for validation\n",
        "      valid_text = flags.valid_file_pretraining.split()\n",
        "      valid_int_text = [vocab_to_int[v] for v in valid_text]\n",
        "      var2 = len(valid_int_text) % flags.batch_size\n",
        "      valid_in_text =  valid_int_text[:] if var2==0 else valid_int_text[:-var2] ## Added -19 to make divisble with 20 batch size\n",
        "      valid_out_text = np.zeros_like(valid_in_text)\n",
        "      valid_out_text[:-1] = valid_in_text[1:]\n",
        "      valid_out_text[-1] = valid_in_text[0]\n",
        "      valid_in_text = np.reshape(valid_in_text, (flags.batch_size, -1))\n",
        "      valid_out_text = np.reshape(valid_out_text, (flags.batch_size, -1))\n",
        "\n",
        "      test_text = flags.test_file_pretraining.split()\n",
        "      test_int_text = [vocab_to_int[v] for v in test_text]\n",
        "      var3 = len(test_int_text) % flags.batch_size\n",
        "      test_in_text =  test_int_text[:] if var3==0 else test_int_text[:-var3]\n",
        "      test_out_text = np.zeros_like(test_in_text)\n",
        "      test_out_text[:-1] = test_in_text[1:]\n",
        "      test_out_text[-1] = test_in_text[0]\n",
        "      test_in_text = np.reshape(test_in_text, (flags.batch_size, -1))\n",
        "      test_out_text = np.reshape(test_out_text, (flags.batch_size, -1))\n",
        "\n",
        "    \n",
        "    if flags.mode == 'friends': \n",
        "      # Load and store data for validation\n",
        "      valid_text = flags.valid_file_friends.split()\n",
        "      valid_int_text = [vocab_to_int[v] for v in valid_text]\n",
        "      var4 = len(valid_int_text) % flags.batch_size\n",
        "      valid_in_text = valid_int_text[:] if  var4==0 else  valid_int_text[:-var4]\n",
        "      valid_out_text = np.zeros_like(valid_in_text)\n",
        "      valid_out_text[:-1] = valid_in_text[1:]\n",
        "      valid_out_text[-1] = valid_in_text[0]\n",
        "      valid_in_text = np.reshape(valid_in_text, (flags.batch_size, -1))\n",
        "      valid_out_text = np.reshape(valid_out_text, (flags.batch_size, -1))\n",
        "\n",
        "      test_text = flags.test_file_friends.split()\n",
        "      test_int_text = [vocab_to_int[v] for v in test_text]\n",
        "      var5 = len(test_int_text) % flags.batch_size\n",
        "      test_in_text = test_int_text[:] if  var5==0 else  test_int_text[:-var5]\n",
        "      test_out_text = np.zeros_like(test_in_text)\n",
        "      test_out_text[:-1] = test_in_text[1:]\n",
        "      test_out_text[-1] = test_in_text[0]\n",
        "      test_in_text = np.reshape(test_in_text, (flags.batch_size, -1))\n",
        "      test_out_text = np.reshape(test_out_text, (flags.batch_size, -1))\n",
        "\n",
        "\n",
        "    if flags.mode == 'pretraining': \n",
        "      net = RNNModule(n_vocab, flags.seq_size,\n",
        "                      flags.embedding_size, flags.lstm_size)\n",
        "      net = net.to(device)\n",
        "\n",
        "    if flags.mode == 'friends':  \n",
        "      net = RNNModule(n_vocab, flags.seq_size,\n",
        "                      flags.embedding_size, flags.lstm_size)\n",
        "      if datafrom=='Drive':\n",
        "        net.load_state_dict(torch.load('/content/drive/My Drive/DL_project/net.pth'))\n",
        "      \n",
        "      net = net.to(device)\n",
        "      net.eval()\n",
        "\n",
        "    criterion, optimizer = get_loss_and_train_op(net, flags.learning_rate)\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "    #Documentation from: https://pytorch.org/docs/master/optim.html\n",
        "\n",
        "    if (flags.schedule_on=='Y'):\n",
        "      expected_number_of_epochs= (np.prod(in_text.shape) // (flags.seq_size * flags.batch_size))*flags.total_epochs\n",
        "      \n",
        "    if (flags.triangular=='Y'):\n",
        "        lambda1= lambda iteration_schedule: triangular_lr_func(iteration_schedule,expected_number_of_epochs)\n",
        "    else:\n",
        "        lambda1= lambda iteration_schedule: easy_schedule(iteration_schedule)\n",
        "\n",
        "\n",
        "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda1)\n",
        "\n",
        "    iteration = 0\n",
        "\n",
        "    if (flags.schedule_on=='Y'):\n",
        "      iteration_schedule=0\n",
        "\n",
        "    epochnumber = []\n",
        "    all_train_losses = []\n",
        "    all_valid_losses = []\n",
        "    all_test_losses = []\n",
        "    for e in range(flags.total_epochs): #Implemented flags.total_epochs to change parameter\n",
        "        epochnumber.extend([e+1])\n",
        "\n",
        "        ######################## TRAINING EPOCH ###########################\n",
        "\n",
        "\n",
        "        ###Variational sequence length###\n",
        "\n",
        "        if (flags.var_seq=='Y'):\n",
        "          base_length=np.round(np.random.normal(flags.seq_size, flags.seq_size/flags.var_seq_std)).astype(int)\n",
        "          if (base_length<2):\n",
        "            base_length=2\n",
        "        else:\n",
        "          base_length=flags.seq_size\n",
        "\n",
        "        #########################\n",
        "\n",
        "\n",
        "\n",
        "        batches = get_batches(in_text, out_text, flags.batch_size, base_length)\n",
        "        valid_batches = get_batches(valid_in_text, valid_out_text, flags.batch_size, base_length)\n",
        "        test_batches = get_batches(test_in_text, test_out_text, flags.batch_size, base_length)\n",
        "\n",
        "        state_h, state_c = net.zero_state(flags.batch_size)\n",
        "        \n",
        "        # Transfer data to GPU\n",
        "        state_h = state_h.to(device)\n",
        "        state_c = state_c.to(device)\n",
        "\n",
        "        for x, y in batches:\n",
        "            iteration += 1\n",
        "\n",
        "\n",
        "            if (flags.schedule_on=='Y'):\n",
        "              iteration_schedule+=1\n",
        "            \n",
        "            # Tell it we are in training mode\n",
        "            net.train()\n",
        "\n",
        "            # Reset all gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Transfer data to GPU\n",
        "            x = torch.tensor(x).to(device)\n",
        "            y = torch.tensor(y).to(device)\n",
        "\n",
        "            logits, (state_h, state_c) = net(x, (state_h, state_c))\n",
        "            loss = criterion(logits.transpose(1, 2), y)\n",
        "\n",
        "            state_h = state_h.detach()\n",
        "            state_c = state_c.detach()\n",
        "\n",
        "            loss_value = loss.item()        \n",
        "\n",
        "            # Perform back-propagation\n",
        "            loss.backward()\n",
        "\n",
        "            _ = torch.nn.utils.clip_grad_norm_(\n",
        "                net.parameters(), flags.gradients_norm)\n",
        "            \n",
        "            # Update the network's parameters\n",
        "            optimizer.step()\n",
        "\n",
        "            #Scheduler step\n",
        "            if (flags.schedule_on=='Y'):\n",
        "              scheduler.step()\n",
        "\n",
        "\n",
        "            if iteration % 100 == 0:\n",
        "                print('Epoch: {}/{}'.format(e, flags.total_epochs), \n",
        "                      'Iteration: {}'.format(iteration),\n",
        "                      '\\t Loss: {}'.format(loss_value))\n",
        "                      #'\\t Perplexity: {}'.format(2**loss_value))\n",
        "        \n",
        "\n",
        "        logits, (state_h, state_c) = net(x, (state_h, state_c))\n",
        "        loss = criterion(logits.transpose(1, 2), y)\n",
        "        loss_value_final = loss.item()    \n",
        "        all_train_losses.append(loss_value_final) \n",
        "        print('Final training loss {}'.format(loss_value_final)) \n",
        "        \n",
        "\n",
        "        ##################### VALIDATION EPOCH ########################\n",
        "        iteration = 0\n",
        "\n",
        "        state_h, state_c = net.zero_state(flags.batch_size)\n",
        "        # Transfer data to GPU\n",
        "\n",
        "        state_h = state_h.to(device)\n",
        "        state_c = state_c.to(device)\n",
        "\n",
        "        for x, y in valid_batches:\n",
        "\n",
        "            iteration += 1\n",
        "\n",
        "            # Tell it we are in evaluation mode\n",
        "            net.eval()\n",
        "\n",
        "            # Transfer data to GPU\n",
        "            x = torch.tensor(x).to(device)\n",
        "            y = torch.tensor(y).to(device)\n",
        "\n",
        "            logits, (state_h, state_c) = net(x, (state_h, state_c))\n",
        "            valid_loss = criterion(logits.transpose(1, 2), y)\n",
        "\n",
        "            state_h = state_h.detach()\n",
        "            state_c = state_c.detach()\n",
        "\n",
        "            valid_loss_value = valid_loss.item()\n",
        "\n",
        "        all_valid_losses.append(valid_loss_value)\n",
        "\n",
        "\n",
        "        ##################### TEST EPOCH ########################\n",
        "        iteration = 0\n",
        "\n",
        "        state_h, state_c = net.zero_state(flags.batch_size)\n",
        "        # Transfer data to GPU\n",
        "\n",
        "        state_h = state_h.to(device)\n",
        "        state_c = state_c.to(device)\n",
        "\n",
        "        for x, y in test_batches:\n",
        "\n",
        "            iteration += 1\n",
        "\n",
        "            # Tell it we are in evaluation mode\n",
        "            net.eval()\n",
        "\n",
        "            # Transfer data to GPU\n",
        "            x = torch.tensor(x).to(device)\n",
        "            y = torch.tensor(y).to(device)\n",
        "\n",
        "            logits, (state_h, state_c) = net(x, (state_h, state_c))\n",
        "            test_loss = criterion(logits.transpose(1, 2), y)\n",
        "\n",
        "            state_h = state_h.detach()\n",
        "            state_c = state_c.detach()\n",
        "\n",
        "            test_loss_value = test_loss.item()\n",
        "\n",
        "        all_test_losses.append(test_loss_value)\n",
        "\n",
        "\n",
        "        print('')\n",
        "       # print('Current mean of validation loss over batches: {}'.format(mean_all_valid_losses))\n",
        "        print('Current validation loss: {}'.format(valid_loss_value))\n",
        "        print('Validation perplexity: {}'.format(np.exp((valid_loss_value))))\n",
        "        print(' ')\n",
        "        print('Current test loss: {}'.format(test_loss_value))\n",
        "        print('Test perplexity: {}'.format(np.exp((test_loss_value))))\n",
        "\n",
        "        print('')\n",
        "            \n",
        "        plt.figure()                             \n",
        "        plt.xlabel('Epochs'), plt.ylabel('Loss')   \n",
        "        plt.plot(epochnumber, all_train_losses, 'r', epochnumber, all_valid_losses, 'b', epochnumber, all_test_losses, 'g')\n",
        "        plt.legend(['Train Loss','Validation Loss', 'Test Loss'])    \n",
        "   \n",
        "        last= predict(device, net, flags.initial_words_train, n_vocab,\n",
        "                            vocab_to_int, int_to_vocab, flags.predict_top_k)\n",
        "        print('\\nThe last predicted 200 characters are:\\n')\n",
        "        print(last[-200:])\n",
        "\n",
        "    #path = \"/content/drive/My Drive/DL_project/\"\n",
        "    # save the pretrained model if the validation loss is less than minimum validation loss.\n",
        "        if flags.mode == 'pretraining' and e > 0 and valid_loss_value <= min(all_valid_losses) and datafrom=='Drive':\n",
        "          torch.save(net.state_dict(), '/content/drive/My Drive/DL_project/net.pth')\n",
        "          print('Saved model')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZE35gpjOaFa",
        "colab_type": "text"
      },
      "source": [
        "##Define a prediction function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6dnXz8s6BSI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(device, net, words, n_vocab, vocab_to_int, int_to_vocab, top_k):\n",
        "    net.eval()\n",
        "\n",
        "    state_h, state_c = net.zero_state(1)\n",
        "    state_h = state_h.to(device)\n",
        "    state_c = state_c.to(device)\n",
        "    for w in words:\n",
        "        ix = torch.tensor([[vocab_to_int[w]]]).to(device)\n",
        "        output, (state_h, state_c) = net(ix, (state_h, state_c))\n",
        "    \n",
        "\n",
        "    _, top_ix = torch.topk(output[0], k=top_k)\n",
        "    choices = top_ix.tolist()\n",
        "    choice = np.random.choice(choices[0]) #Looks like a way to avoid always choose like \"and\" \"then\"..... \n",
        "\n",
        "    words.append(int_to_vocab[choice])\n",
        "\n",
        "    for _ in range(100):\n",
        "        ix = torch.tensor([[choice]]).to(device)\n",
        "        output, (state_h, state_c) = net(ix, (state_h, state_c))\n",
        "\n",
        "        _, top_ix = torch.topk(output[0], k=top_k)\n",
        "        choices = top_ix.tolist()\n",
        "        choice = np.random.choice(choices[0])\n",
        "        words.append(int_to_vocab[choice])\n",
        "\n",
        "    print(' '.join(words))\n",
        "    return ' '.join(words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rp4Z_Us8OfT6",
        "colab_type": "text"
      },
      "source": [
        "#***Run model***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKG_14IA6JCF",
        "colab_type": "code",
        "outputId": "10139987-1cb2-4466-b38c-26e98d7deb85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        }
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------  \n",
            "----------------------Go into training mode--------------------------------  \n",
            "----------------------using friends data------------------------\n",
            "------------------------------------------------------------------------  \n",
            "Vocabulary size 17135\n",
            "Epoch: 0/100 Iteration: 100 \t Loss: 5.9932708740234375\n",
            "Epoch: 0/100 Iteration: 200 \t Loss: 6.00351619720459\n",
            "Epoch: 0/100 Iteration: 300 \t Loss: 5.975262641906738\n",
            "Epoch: 0/100 Iteration: 400 \t Loss: 5.628751277923584\n",
            "Epoch: 0/100 Iteration: 500 \t Loss: 5.6936187744140625\n",
            "Epoch: 0/100 Iteration: 600 \t Loss: 5.8841233253479\n",
            "Epoch: 0/100 Iteration: 700 \t Loss: 5.535355567932129\n",
            "Epoch: 0/100 Iteration: 800 \t Loss: 5.4795660972595215\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-140-c7bc734e5e35>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-138-d175a66685ff>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0mstate_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate_c\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m             \u001b[0mloss_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0;31m# Perform back-propagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}