{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Merge af NLP Friends.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "PhzDijSPm9no"
      ],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LauJohansson/DeepLearning_NLP_Friends/blob/master/Merge_af_NLP_Friends.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YiIMKIBRmeI_",
        "colab_type": "text"
      },
      "source": [
        "#**Introduction**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twGMoGa7mhCd",
        "colab_type": "text"
      },
      "source": [
        "We build a Recurrent Neural Network (RNN) using the Long Short-Term Memory (LSTM) architecture. Through tuning of hyperparameters and regularization through variational sequence length and DropConnect we achieve a perplexity value on the Penn Treebank dataset. We then utilize the found model to generate dialogue for the TV-series “Friends”. To this end, we pretrain the model on Wikipedia text and further tune the hyperparameters to fit “Friends”. Finally, we show a subset of the predicted dialogue and visualise the found semantics of “Friends” through t-SNE on the embedding weights.  \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LtIKDkJSg2XV",
        "colab_type": "text"
      },
      "source": [
        "NB: When opening from Github you can not load or save pretrained model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhzDijSPm9no",
        "colab_type": "text"
      },
      "source": [
        "##Acknowledgements"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wX9wD9wbnJSY",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "This notebook is inspired by the following NLP Tutorial:https://github.com/graykode/nlp-tutorial\n",
        "Made by Tae-Hwan Jung. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DK8RFoquSj5",
        "colab_type": "text"
      },
      "source": [
        "# **Initialising Friends data**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLpYqib8PeGH",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://drive.google.com/uc?id=1JzPdBavHFL-8R1Tc0F6z9KlELR326MXy)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84ed6CSdPlRY",
        "colab_type": "code",
        "outputId": "1fc5ab5c-d878-4e08-82cd-71903bf2da7c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import collections\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import glob\n",
        "import io\n",
        "import matplotlib. pyplot as plt\n",
        "from collections import Counter\n",
        "# Importing drive method from colab for accessing google drive\n",
        "from google.colab import drive\n",
        "\n",
        "# Mounting drive\n",
        "# This will require authentication : Follow the steps as guided\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "try: #If files in google drive\n",
        "  datafrom='Drive'\n",
        "  friends_train = open(\"/content/drive/My Drive/DL_project/friends_train.txt\").read()\n",
        "  friends_valid = open(\"/content/drive/My Drive/DL_project/friends_valid.txt\").read()\n",
        "  friends_test = open(\"/content/drive/My Drive/DL_project/friends_test.txt\").read()\n",
        "\n",
        "  pretraining_train = open(\"/content/drive/My Drive/DL_project/wikitrainclean.txt\").read()\n",
        "  pretraining_valid = open(\"/content/drive/My Drive/DL_project/wikivalidclean.txt\").read()\n",
        "  pretraining_test = open(\"/content/drive/My Drive/DL_project/wikitestclean.txt\").read()\n",
        "\n",
        "  \n",
        "except: #For github version\n",
        "  datafrom='Github'\n",
        "  from urllib.request import urlopen\n",
        "\n",
        "  friends_train=str(urlopen('https://raw.githubusercontent.com/LauJohansson/DeepLearning_NLP_Friends/master/Data/friends_train.txt').read(),encoding=\"utf-8\")\n",
        "  friends_valid = str(urlopen('https://raw.githubusercontent.com/LauJohansson/DeepLearning_NLP_Friends/master/Data/friends_valid.txt').read(),encoding=\"utf-8\")\n",
        "  friends_test = str(urlopen('https://raw.githubusercontent.com/LauJohansson/DeepLearning_NLP_Friends/master/Data/friends_test.txt').read(),encoding=\"utf-8\")\n",
        "\n",
        "  pretraining_train = str(urlopen('https://raw.githubusercontent.com/LauJohansson/DeepLearning_NLP_Friends/master/Data/seinfeldtrainclean.txt').read(),encoding=\"utf-8\")\n",
        "  pretraining_valid = str(urlopen('https://raw.githubusercontent.com/LauJohansson/DeepLearning_NLP_Friends/master/Data/seinfeldvalidclean.txt').read(),encoding=\"utf-8\")\n",
        "  pretraining_test = str(urlopen('https://raw.githubusercontent.com/LauJohansson/DeepLearning_NLP_Friends/master/Data/seinfeldtestclean.txt').read(),encoding=\"utf-8\")\n"
      ],
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIQl-Df3TiEZ",
        "colab_type": "code",
        "outputId": "6cf8ac39-6fdc-4ae9-b695-498ea6dc833e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "# check data\n",
        "print(\"Traindata snippet friends:    \" + friends_train[:100])\n",
        "print(\"Validdata snippet friends:    \" + friends_valid[:100])\n",
        "print(\"Testdata snippet friends:     \" + friends_test[:100])\n",
        "print()\n",
        "print(\"Traindata snippet pretraining:       \" + pretraining_train[:100])\n",
        "print(\"Validdata snippet pretraining:       \" + pretraining_valid[:100])\n",
        "print(\"Testdata snippet pretraining:        \" + pretraining_test[:100])"
      ],
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Traindata snippet friends:    [ scene: central perk , chandler , joey , phoebe , and monica are there . ] monica: there's nothing \n",
            "Validdata snippet friends:    parts that i really wanted . you always believed in me man . even , even when i didn't believe in my\n",
            "Testdata snippet friends:     is at the half - opened door ] phoebe: ( in a strange heavy accent ) hello <unk> , it's time for you\n",
            "\n",
            "Traindata snippet pretraining:       valkyria chronicles iii senj\\u014d no valkyria 3 : <unk> chronicles ( japanese : \\u6226\\u5834\\u306e\\\n",
            "Validdata snippet pretraining:       homarus gammarus homarus gammarus , known as the european lobster or common lobster , is a species o\n",
            "Testdata snippet pretraining:        robert <unk> robert <unk> is an english film , television and theatre actor . he had a guest @ - @ s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvq6sH9HSqtx",
        "colab_type": "code",
        "outputId": "a269339a-6adc-49b0-ae4d-ea44ea421c4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Find words that occur in friends data, but not in pretraining data\n",
        "words_notinpretraining = list(set(friends_train.split()).difference(pretraining_train.split()))\n",
        "words_notinpretraining[0:5]"
      ],
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"nana's\", 'bulge', 'shoulda', 'botched', 'aspirin']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 144
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBJq1zyuuGkz",
        "colab_type": "text"
      },
      "source": [
        "# **Initialising Penn Tree Bank data**\n",
        "![alt text](https://drive.google.com/uc?id=1A8SY9dmsLx2hoanWWWhT_IJnJcJOmluR)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAp6BEbqQEDe",
        "colab_type": "text"
      },
      "source": [
        "The following site is used as inspiration to get PTB data: https://corochann.com/penn-tree-bank-ptb-dataset-introduction-1456.html\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zoW2qb_YQMil",
        "colab_type": "code",
        "outputId": "6878afaa-3a36-4960-d081-b34e2e304d5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "'''\n",
        "#Get PTB data\n",
        "from __future__ import print_function\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        " \n",
        " \n",
        "import numpy as np\n",
        " \n",
        "import chainer\n",
        "train, val, test = chainer.datasets.get_ptb_words()\n",
        "\n",
        "ptb_dict = chainer.datasets.get_ptb_words_vocabulary()\n",
        "print('Number of vocabulary', len(ptb_dict))\n",
        "print('ptb_dict', ptb_dict)\n",
        "\n",
        "ptb_word_id_dict = ptb_dict\n",
        "ptb_id_word_dict = dict((v,k) for k,v in ptb_word_id_dict.items())\n",
        "\n",
        "\n",
        "#Create data as str\n",
        "dataPTB=' '.join([ptb_id_word_dict[i] for i in train[:]])\n",
        "dataPTBtest=' '.join([ptb_id_word_dict[i] for i in test[:]])\n",
        "dataPTBvalid=' '.join([ptb_id_word_dict[i] for i in val[:]])\n",
        "'''"
      ],
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n#Get PTB data\\nfrom __future__ import print_function\\nimport os\\nimport matplotlib.pyplot as plt\\n%matplotlib inline\\n \\n \\nimport numpy as np\\n \\nimport chainer\\ntrain, val, test = chainer.datasets.get_ptb_words()\\n\\nptb_dict = chainer.datasets.get_ptb_words_vocabulary()\\nprint('Number of vocabulary', len(ptb_dict))\\nprint('ptb_dict', ptb_dict)\\n\\nptb_word_id_dict = ptb_dict\\nptb_id_word_dict = dict((v,k) for k,v in ptb_word_id_dict.items())\\n\\n\\n#Create data as str\\ndataPTB=' '.join([ptb_id_word_dict[i] for i in train[:]])\\ndataPTBtest=' '.join([ptb_id_word_dict[i] for i in test[:]])\\ndataPTBvalid=' '.join([ptb_id_word_dict[i] for i in val[:]])\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 145
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIVP8LXaNyGA",
        "colab_type": "text"
      },
      "source": [
        "#***Import and initializing hyperparameters***\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2otEIPN5TPc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import os\n",
        "from argparse import Namespace\n",
        "\n",
        "flags = Namespace(\n",
        "    mode = 'friends', #'pretraining' is pretraining and 'friends' is fine-tuning.\n",
        "    name='Friends',\n",
        "    train_file_pretraining = pretraining_train,                    #shift between data (data or dataPTB)\n",
        "    valid_file_pretraining = pretraining_valid, \n",
        "    test_file_pretraining = pretraining_test,   \n",
        "    train_file_friends= friends_train,                    #shift between data (data or dataPTB)\n",
        "    valid_file_friends= friends_valid, \n",
        "    test_file_friends = friends_test,\n",
        "    seq_size= 32, #16,\n",
        "    batch_size=40, #16,\n",
        "    embedding_size=256, #300,\n",
        "    lstm_size=512, #,300,                      #Hidden size\n",
        "    gradients_norm=0.5,\n",
        "    #initial_words=['banknote', 'berlitz'],\n",
        "    initial_words_train = [],\n",
        "    initial_words_valid= [],\n",
        "    predict_top_k=5,                    #Choose the k best next_word_prediction, and a random is chosing.\n",
        "    checkpoint_path='checkpoint',\n",
        "    total_epochs=100,   #30                  #Change number of epochs in training\n",
        "    learning_rate=0.001,\n",
        "    predict_every=1000,\n",
        "    #validation_corpus_size=len(valid_file.split()),\n",
        "    dropconnect_rate=0.4,\n",
        "    n_lay=2,\n",
        "\n",
        "    #Set variational sequence length on/off\n",
        "    var_seq='Y',                            #Choose if variational sequence length is on/off\n",
        "    var_seq_std=2,                          #Choose std. dev. for norm distribution for var. seq. length ( in moment 1/2 of seq length)\n",
        "\n",
        "\n",
        "    #scheduler parameters\n",
        "    schedule_on='N',                        #Choose if LR-scheduler is on/off\n",
        "    triangular='N',                         #Choose 'Y' to turn on the slanted triangular LR. \n",
        "    cut_fracI=0.2,                          #Choose the fraction of iterations we increase the LR in STL\n",
        "    ratioI=32,                              #Choose how much smaller the lowest LR is from the maximum LR ηmax\n",
        "    nmaxI=0.0,                              #this will be set = learning_rate \n",
        "\n",
        "    #Use same drop-mask for drop-connect\n",
        "    same_drop_lstm='N',                     #Choose 'Y' if drop-connect all should use same mask\n",
        "    \n",
        "    #Dropout on embedding layer\n",
        "    drop_embed=0.5,                         #Choose dropoutrate for embedding dropout        \n",
        "\n",
        "    #Optimizer selection\n",
        "    optim_select='AdamW'                      #Choose between \"AdamW, SGD, ASGD\"\n",
        "\n",
        ")\n",
        "\n",
        "\n",
        "#initialising two random words, the train and test data should use\n",
        "if flags.mode == 'friends':\n",
        "  for i in range(0,1):\n",
        "    flags.initial_words_train.append(np.random.choice(flags.train_file_friends.split()))\n",
        "    flags.initial_words_valid.append(np.random.choice(flags.valid_file_friends.split()))\n",
        "    flags.validation_corpus_size = len(flags.valid_file_friends.split())\n",
        "    \n",
        "if flags.mode == 'pretraining':\n",
        "  for i in range(0,1):\n",
        "    flags.initial_words_train.append(np.random.choice(flags.train_file_pretraining.split()))\n",
        "    flags.initial_words_valid.append(np.random.choice(flags.valid_file_pretraining.split()))\n",
        "    flags.validation_corpus_size = len(flags.valid_file_pretraining.split())\n",
        "\n",
        "flags.nmaxI=flags.learning_rate"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKozVznDlxMS",
        "colab_type": "text"
      },
      "source": [
        "#**Define functions and set up model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYSUF18ZN6cf",
        "colab_type": "text"
      },
      "source": [
        "##Function to get data for main and batch-function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjlDy7cq5UA8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_data_from_file(train_file, batch_size, seq_size):\n",
        "  \n",
        "    text=train_file.split()\n",
        "    # Extend words_notinpretraining to text to get them as a part of the mapping dictionary\n",
        "    text.extend(words_notinpretraining)\n",
        "    word_counts = Counter(text)\n",
        "    sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
        "    int_to_vocab = {k: w for k, w in enumerate(sorted_vocab)}\n",
        "    vocab_to_int = {w: k for k, w in int_to_vocab.items()}\n",
        "    n_vocab = len(int_to_vocab)\n",
        "\n",
        "    text=train_file.split()\n",
        "\n",
        "    print('Vocabulary size', n_vocab)\n",
        "\n",
        "    int_text = [vocab_to_int[w] for w in text]\n",
        "    num_batches = int(len(int_text) / (seq_size * batch_size))\n",
        "    in_text = int_text[:num_batches * batch_size * seq_size]\n",
        "    out_text = np.zeros_like(in_text)\n",
        "    out_text[:-1] = in_text[1:]\n",
        "    out_text[-1] = in_text[0]\n",
        "    in_text = np.reshape(in_text, (batch_size, -1))\n",
        "    out_text = np.reshape(out_text, (batch_size, -1))\n",
        "    return int_to_vocab, vocab_to_int, n_vocab, in_text, out_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7pPsjVmq5Wes",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_batches(in_text, out_text, batch_size, seq_size):\n",
        "    num_batches = np.prod(in_text.shape) // (seq_size * batch_size)\n",
        "    for i in range(0, num_batches * seq_size, seq_size):\n",
        "        yield in_text[:, i:i+seq_size], out_text[:, i:i+seq_size]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VkuuPkykOH6k",
        "colab_type": "text"
      },
      "source": [
        "##Setting up the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncUCPVoc5bLd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RNNModule(nn.Module):\n",
        "    def __init__(self, n_vocab, seq_size, embedding_size, lstm_size):\n",
        "        super(RNNModule, self).__init__()\n",
        "        self.seq_size = seq_size\n",
        "        self.lstm_size = lstm_size\n",
        "        self.embedding = nn.Embedding(n_vocab, embedding_size)\n",
        "        self.lstm=nn.LSTM(embedding_size,\n",
        "                            lstm_size,\n",
        "                            batch_first=True,num_layers=flags.n_lay)\n",
        "        \n",
        "        self.dense = nn.Linear(lstm_size, n_vocab)\n",
        "        self.drop_out=nn.Dropout(flags.drop_embed)\n",
        "        self.OneMaskOnly=torch.autograd.Variable(torch.Tensor(getattr(self.lstm,self.lstm._all_weights[0][0]).shape[0],\n",
        "                                                        getattr(self.lstm,self.lstm._all_weights[0][0]).shape[1]).uniform_().to(\"cuda\") > flags.dropconnect_rate)\n",
        "\n",
        "\n",
        "    def forward(self, x, prev_state):\n",
        "        embed = self.embedding(x)\n",
        "        embed=self.drop_out(embed)\n",
        "        orig=[]\n",
        "\n",
        "        #Make dropconnect\n",
        "        if self.training:\n",
        "          for i in range( len(self.lstm._all_weights[0])):\n",
        "            name = self.lstm._all_weights[0][i]\n",
        "            if name.find('LSTM.weight_hh_l')!=-1:      \n",
        "              orig.append(getattr(self.lstm,name))\n",
        "            \n",
        "              if flags.same_drop_lstm=='Y':\n",
        "                mask=self.OneMaskOnly\n",
        "              else:\n",
        "                mask=torch.autograd.Variable(torch.Tensor(getattr(self.lstm,name).shape[0],\n",
        "                                                        getattr(self.lstm,name).shape[1]).uniform_().to(\"cuda\") > flags.flags.dropconnect_rate)\n",
        "              setattr(self.lstm,name,torch.nn.Parameter(torch.mul(getattr(self.lstm,name),mask)))\n",
        "              \n",
        "              self.lstm.flatten_parameters()\n",
        "             \n",
        "\n",
        "        #LSTM forward\n",
        "        output, state = self.lstm(embed, prev_state)\n",
        "\n",
        "        #Set hh-weight back to original\n",
        "        if self.training:\n",
        "          a=0\n",
        "          for i in range( len(self.lstm._all_weights[0])):\n",
        "            name = self.lstm._all_weights[0][i]\n",
        "            if name.find('LSTM.weight_hh_l')!=-1:\n",
        "              print(orig)\n",
        "              setattr(self.lstm,name,orig[a])\n",
        "              #self.lstm.weight_hh_l0=orig\n",
        "              self.lstm.flatten_parameters()\n",
        "              a=+1\n",
        "\n",
        "        logits = self.dense(output)\n",
        "\n",
        "        return logits, state\n",
        "       \n",
        "    def zero_state(self, batch_size):\n",
        "        return (torch.zeros(flags.n_lay, batch_size, self.lstm_size),\n",
        "                torch.zeros(flags.n_lay, batch_size, self.lstm_size))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kf20oH-l9pIZ",
        "colab_type": "text"
      },
      "source": [
        "##Defining ASGD\n",
        "from: https://pytorch.org/docs/stable/_modules/torch/optim/asgd.html\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63Qg4X449uXi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import torch\n",
        "#from .optimizer import Optimizer\n",
        "\n",
        "\n",
        "class ASGD(torch.optim.Optimizer):\n",
        "    \"\"\"Implements Averaged Stochastic Gradient Descent.\n",
        "\n",
        "    It has been proposed in `Acceleration of stochastic approximation by\n",
        "    averaging`_.\n",
        "\n",
        "    Arguments:\n",
        "        params (iterable): iterable of parameters to optimize or dicts defining\n",
        "            parameter groups\n",
        "        lr (float, optional): learning rate (default: 1e-2)\n",
        "        lambd (float, optional): decay term (default: 1e-4)\n",
        "        alpha (float, optional): power for eta update (default: 0.75)\n",
        "        t0 (float, optional): point at which to start averaging (default: 1e6)\n",
        "        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n",
        "\n",
        "    .. _Acceleration of stochastic approximation by averaging:\n",
        "        http://dl.acm.org/citation.cfm?id=131098\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, params, lr=1e-2, lambd=1e-4, alpha=0.75, t0=1e6, weight_decay=0):\n",
        "        if not 0.0 <= lr:\n",
        "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
        "        if not 0.0 <= weight_decay:\n",
        "            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n",
        "\n",
        "        defaults = dict(lr=lr, lambd=lambd, alpha=alpha, t0=t0,\n",
        "                        weight_decay=weight_decay)\n",
        "        super(ASGD, self).__init__(params, defaults)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        \"\"\"Performs a single optimization step.\n",
        "\n",
        "        Arguments:\n",
        "            closure (callable, optional): A closure that reevaluates the model\n",
        "                and returns the loss.\n",
        "        \"\"\"\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                grad = p.grad.data\n",
        "                if grad.is_sparse:\n",
        "                    raise RuntimeError('ASGD does not support sparse gradients')\n",
        "                state = self.state[p]\n",
        "\n",
        "                # State initialization\n",
        "                if len(state) == 0:\n",
        "                    state['step'] = 0\n",
        "                    state['eta'] = group['lr']\n",
        "                    state['mu'] = 1\n",
        "                    state['ax'] = torch.zeros_like(p.data)\n",
        "\n",
        "                state['step'] += 1\n",
        "\n",
        "                if group['weight_decay'] != 0:\n",
        "                    grad = grad.add(group['weight_decay'], p.data)\n",
        "\n",
        "                # decay term\n",
        "                p.data.mul_(1 - group['lambd'] * state['eta'])\n",
        "\n",
        "                # update parameter\n",
        "                p.data.add_(-state['eta'], grad)\n",
        "\n",
        "                # averaging\n",
        "                if state['mu'] != 1:\n",
        "                    state['ax'].add_(p.data.sub(state['ax']).mul(state['mu']))\n",
        "                else:\n",
        "                    state['ax'].copy_(p.data)\n",
        "\n",
        "                # update eta and mu\n",
        "                state['eta'] = (group['lr'] /\n",
        "                                math.pow((1 + group['lambd'] * group['lr'] * state['step']), group['alpha']))\n",
        "                state['mu'] = 1 / max(1, state['step'] - group['t0'])\n",
        "\n",
        "        return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gb22lHosOTjd",
        "colab_type": "text"
      },
      "source": [
        "##Define function to call optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZl2BaWK5mRx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_loss_and_train_op(net, lrI):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    if (flags.optim_select=='ASGD'):\n",
        "      optimizer = torch.optim.ASGD(net.parameters(), lr=lrI,weight_decay=0,t0=0,lambd=0) #decay, t0 and lambd taken from article\n",
        "    elif (flags.optim_select=='SGD'):\n",
        "      optimizer = torch.optim.SGD(net.parameters(), lr=lrI) #decay, t0 and lambd taken from article\n",
        "    else:  \n",
        "      optimizer = torch.optim.AdamW(net.parameters(), lr=lrI,weight_decay=0.3)\n",
        "  \n",
        "    return criterion, optimizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uy8anIthSTXZ",
        "colab_type": "text"
      },
      "source": [
        "##Define function to schedule optimizer for ASGD\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Lqz2wRTUsAw",
        "colab_type": "text"
      },
      "source": [
        "Article used: [Universal Language Model Fine-tuning for Text Classification](https://www.aclweb.org/anthology/P18-1031.pdf?fbclid=IwAR0-TADs3LWh74b4xbA2QW5OYM5-_5iFu2EBjd_0-KVWOUytnBV5TeS9KGo)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dC3LvPuXSTqm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#T=number of epochs * number of updates per epoch\n",
        "#\"cut_frac\" is the fraction of iterations we increase the LR\n",
        "# \"cut\" is the iteration when we switch from increasing to decreasing the LR\n",
        "# \"p\" is the fraction of the number of iterations we have increased or will decrease the LR respectively\n",
        "# \"ratio\" specifies how much smaller the lowest LR is from the maximum LR nmax\n",
        "#\"nt\"/return value is the learning rate at iteration t.\n",
        "\n",
        "#The paper generally use cut_frac = 0.1, ratio = 32 and ηmax = 0.01\n",
        "\n",
        "\n",
        "def triangular_lr_func(t,T,cut_frac=flags.cut_fracI, ratio=flags.ratioI, nmax=flags.nmaxI ):\n",
        "    cut=np.floor(T*cut_frac)\n",
        "\n",
        "    if (t<cut): p=t/cut\n",
        "    else: p=1-((t-cut)/(cut*(ratio-1)))\n",
        "\n",
        "\n",
        "    return nmax * (1+p*(ratio-1))/ratio"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-OSrXMTeA6i",
        "colab_type": "text"
      },
      "source": [
        "##Custommade scheduler\n",
        "This scheduler is an example on how ASGD LR can be decreased for each iteration. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9aq4WV5lLes_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def easy_schedule(t,nmax=flags.nmaxI):\n",
        "  result = nmax-(t*0.0001)\n",
        "\n",
        "  if(result<0.1): result=0.1\n",
        "\n",
        "  return result\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXgqNYjZOW9X",
        "colab_type": "text"
      },
      "source": [
        "#**Define main**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNF3L6ss5qHL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main():\n",
        "    torch.manual_seed(11)\n",
        "    print('------------------------------------------------------------------------  ')\n",
        "    print('----------------------Go into training mode--------------------------------  ')\n",
        "    print(\"----------------------using %s data------------------------\" % flags.mode)\n",
        "    print('------------------------------------------------------------------------  ')\n",
        "    \n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    # Pre-training mode on pretrainingtext\n",
        "    # Start network from scratch\n",
        "    if flags.mode == 'pretraining': \n",
        "      int_to_vocab, vocab_to_int, n_vocab, in_text, out_text = get_data_from_file(\n",
        "          flags.train_file_pretraining, flags.batch_size, flags.seq_size)\n",
        "           \n",
        "    # Fine-tuning mode on friends\n",
        "    if flags.mode == 'friends': \n",
        "\n",
        "      # Get int_to_vocab, vocab_to_int, n_vocab from pretraining text pre-training\n",
        "      int_to_vocab, vocab_to_int, n_vocab, _, _ = get_data_from_file( \n",
        "          flags.train_file_pretraining, flags.batch_size, flags.seq_size)\n",
        "      \n",
        "      # Load and store data for friends-training\n",
        "      text = flags.train_file_friends.split()\n",
        "      int_text = [vocab_to_int[v] for v in text]\n",
        "      var1 = len(int_text) % flags.batch_size \n",
        "      in_text =   int_text[:] if var1==0 else  int_text[:-var1]              ## Added -19 to make divisble with 20 batch size\n",
        "      out_text = np.zeros_like(in_text)\n",
        "      out_text[:-1] = in_text[1:]\n",
        "      out_text[-1] = in_text[0]\n",
        "      in_text = np.reshape(in_text, (flags.batch_size, -1))\n",
        "      out_text = np.reshape(out_text, (flags.batch_size, -1))\n",
        "\n",
        "\n",
        "    if flags.mode == 'pretraining': \n",
        "      # Load and store data for validation\n",
        "      valid_text = flags.valid_file_pretraining.split()\n",
        "      valid_int_text = [vocab_to_int[v] for v in valid_text]\n",
        "      var2 = len(valid_int_text) % flags.batch_size\n",
        "      valid_in_text =  valid_int_text[:] if var2==0 else valid_int_text[:-var2] ## Added -19 to make divisble with 20 batch size\n",
        "      valid_out_text = np.zeros_like(valid_in_text)\n",
        "      valid_out_text[:-1] = valid_in_text[1:]\n",
        "      valid_out_text[-1] = valid_in_text[0]\n",
        "      valid_in_text = np.reshape(valid_in_text, (flags.batch_size, -1))\n",
        "      valid_out_text = np.reshape(valid_out_text, (flags.batch_size, -1))\n",
        "\n",
        "      test_text = flags.test_file_pretraining.split()\n",
        "      test_int_text = [vocab_to_int[v] for v in test_text]\n",
        "      var3 = len(test_int_text) % flags.batch_size\n",
        "      test_in_text =  test_int_text[:] if var3==0 else test_int_text[:-var3]\n",
        "      test_out_text = np.zeros_like(test_in_text)\n",
        "      test_out_text[:-1] = test_in_text[1:]\n",
        "      test_out_text[-1] = test_in_text[0]\n",
        "      test_in_text = np.reshape(test_in_text, (flags.batch_size, -1))\n",
        "      test_out_text = np.reshape(test_out_text, (flags.batch_size, -1))\n",
        "\n",
        "    \n",
        "    if flags.mode == 'friends': \n",
        "      # Load and store data for validation\n",
        "      valid_text = flags.valid_file_friends.split()\n",
        "      valid_int_text = [vocab_to_int[v] for v in valid_text]\n",
        "      var4 = len(valid_int_text) % flags.batch_size\n",
        "      valid_in_text = valid_int_text[:] if  var4==0 else  valid_int_text[:-var4]\n",
        "      valid_out_text = np.zeros_like(valid_in_text)\n",
        "      valid_out_text[:-1] = valid_in_text[1:]\n",
        "      valid_out_text[-1] = valid_in_text[0]\n",
        "      valid_in_text = np.reshape(valid_in_text, (flags.batch_size, -1))\n",
        "      valid_out_text = np.reshape(valid_out_text, (flags.batch_size, -1))\n",
        "\n",
        "      test_text = flags.test_file_friends.split()\n",
        "      test_int_text = [vocab_to_int[v] for v in test_text]\n",
        "      var5 = len(test_int_text) % flags.batch_size\n",
        "      test_in_text = test_int_text[:] if  var5==0 else  test_int_text[:-var5]\n",
        "      test_out_text = np.zeros_like(test_in_text)\n",
        "      test_out_text[:-1] = test_in_text[1:]\n",
        "      test_out_text[-1] = test_in_text[0]\n",
        "      test_in_text = np.reshape(test_in_text, (flags.batch_size, -1))\n",
        "      test_out_text = np.reshape(test_out_text, (flags.batch_size, -1))\n",
        "\n",
        "\n",
        "    if flags.mode == 'pretraining': \n",
        "      net = RNNModule(n_vocab, flags.seq_size,\n",
        "                      flags.embedding_size, flags.lstm_size)\n",
        "      net = net.to(device)\n",
        "\n",
        "    if flags.mode == 'friends':  \n",
        "      net = RNNModule(n_vocab, flags.seq_size,\n",
        "                      flags.embedding_size, flags.lstm_size)\n",
        "      if datafrom=='Drive':\n",
        "        net.load_state_dict(torch.load('/content/drive/My Drive/DL_project/net.pth'))\n",
        "      else:\n",
        "        net.load_state_dict(torch.load('/content/drive/My Drive/net.pth'))\n",
        "      \n",
        "      net = net.to(device)\n",
        "      net.eval()\n",
        "\n",
        "    criterion, optimizer = get_loss_and_train_op(net, flags.learning_rate)\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "    #Documentation from: https://pytorch.org/docs/master/optim.html\n",
        "\n",
        "    if (flags.schedule_on=='Y'):\n",
        "      expected_number_of_epochs= (np.prod(in_text.shape) // (flags.seq_size * flags.batch_size))*flags.total_epochs\n",
        "      \n",
        "    if (flags.triangular=='Y'):\n",
        "        lambda1= lambda iteration_schedule: triangular_lr_func(iteration_schedule,expected_number_of_epochs)\n",
        "    else:\n",
        "        lambda1= lambda iteration_schedule: easy_schedule(iteration_schedule)\n",
        "\n",
        "\n",
        "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda1)\n",
        "\n",
        "    iteration = 0\n",
        "\n",
        "    if (flags.schedule_on=='Y'):\n",
        "      iteration_schedule=0\n",
        "\n",
        "    epochnumber = []\n",
        "    all_train_losses = []\n",
        "    all_valid_losses = []\n",
        "    all_test_losses = []\n",
        "    for e in range(flags.total_epochs): #Implemented flags.total_epochs to change parameter\n",
        "        epochnumber.extend([e+1])\n",
        "\n",
        "        ######################## TRAINING EPOCH ###########################\n",
        "\n",
        "\n",
        "        ###Variational sequence length###\n",
        "\n",
        "        if (flags.var_seq=='Y'):\n",
        "          base_length=np.round(np.random.normal(flags.seq_size, flags.seq_size/flags.var_seq_std)).astype(int)\n",
        "          if (base_length<2):\n",
        "            base_length=2\n",
        "        else:\n",
        "          base_length=flags.seq_size\n",
        "\n",
        "        #########################\n",
        "\n",
        "\n",
        "\n",
        "        batches = get_batches(in_text, out_text, flags.batch_size, base_length)\n",
        "        valid_batches = get_batches(valid_in_text, valid_out_text, flags.batch_size, base_length)\n",
        "        test_batches = get_batches(test_in_text, test_out_text, flags.batch_size, base_length)\n",
        "\n",
        "        state_h, state_c = net.zero_state(flags.batch_size)\n",
        "        \n",
        "        # Transfer data to GPU\n",
        "        state_h = state_h.to(device)\n",
        "        state_c = state_c.to(device)\n",
        "\n",
        "        for x, y in batches:\n",
        "            iteration += 1\n",
        "\n",
        "\n",
        "            if (flags.schedule_on=='Y'):\n",
        "              iteration_schedule+=1\n",
        "            \n",
        "            # Tell it we are in training mode\n",
        "            net.train()\n",
        "\n",
        "            # Reset all gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Transfer data to GPU\n",
        "            x = torch.tensor(x).to(device)\n",
        "            y = torch.tensor(y).to(device)\n",
        "\n",
        "            logits, (state_h, state_c) = net(x, (state_h, state_c))\n",
        "            loss = criterion(logits.transpose(1, 2), y)\n",
        "\n",
        "            state_h = state_h.detach()\n",
        "            state_c = state_c.detach()\n",
        "\n",
        "            loss_value = loss.item()        \n",
        "\n",
        "            # Perform back-propagation\n",
        "            loss.backward()\n",
        "\n",
        "            _ = torch.nn.utils.clip_grad_norm_(\n",
        "                net.parameters(), flags.gradients_norm)\n",
        "            \n",
        "            # Update the network's parameters\n",
        "            optimizer.step()\n",
        "\n",
        "            #Scheduler step\n",
        "            if (flags.schedule_on=='Y'):\n",
        "              scheduler.step()\n",
        "\n",
        "\n",
        "            if iteration % 100 == 0:\n",
        "                print('Epoch: {}/{}'.format(e, flags.total_epochs), \n",
        "                      'Iteration: {}'.format(iteration),\n",
        "                      '\\t Loss: {}'.format(loss_value))\n",
        "                      #'\\t Perplexity: {}'.format(2**loss_value))\n",
        "        \n",
        "\n",
        "        logits, (state_h, state_c) = net(x, (state_h, state_c))\n",
        "        loss = criterion(logits.transpose(1, 2), y)\n",
        "        loss_value_final = loss.item()    \n",
        "        all_train_losses.append(loss_value_final) \n",
        "        print('Final training loss {}'.format(loss_value_final)) \n",
        "        \n",
        "\n",
        "        ##################### VALIDATION EPOCH ########################\n",
        "        iteration = 0\n",
        "\n",
        "        state_h, state_c = net.zero_state(flags.batch_size)\n",
        "        # Transfer data to GPU\n",
        "\n",
        "        state_h = state_h.to(device)\n",
        "        state_c = state_c.to(device)\n",
        "\n",
        "        for x, y in valid_batches:\n",
        "\n",
        "            iteration += 1\n",
        "\n",
        "            # Tell it we are in evaluation mode\n",
        "            net.eval()\n",
        "\n",
        "            # Transfer data to GPU\n",
        "            x = torch.tensor(x).to(device)\n",
        "            y = torch.tensor(y).to(device)\n",
        "\n",
        "            logits, (state_h, state_c) = net(x, (state_h, state_c))\n",
        "            valid_loss = criterion(logits.transpose(1, 2), y)\n",
        "\n",
        "            state_h = state_h.detach()\n",
        "            state_c = state_c.detach()\n",
        "\n",
        "            valid_loss_value = valid_loss.item()\n",
        "\n",
        "        all_valid_losses.append(valid_loss_value)\n",
        "\n",
        "\n",
        "        ##################### TEST EPOCH ########################\n",
        "        iteration = 0\n",
        "\n",
        "        state_h, state_c = net.zero_state(flags.batch_size)\n",
        "        # Transfer data to GPU\n",
        "\n",
        "        state_h = state_h.to(device)\n",
        "        state_c = state_c.to(device)\n",
        "\n",
        "        for x, y in test_batches:\n",
        "\n",
        "            iteration += 1\n",
        "\n",
        "            # Tell it we are in evaluation mode\n",
        "            net.eval()\n",
        "\n",
        "            # Transfer data to GPU\n",
        "            x = torch.tensor(x).to(device)\n",
        "            y = torch.tensor(y).to(device)\n",
        "\n",
        "            logits, (state_h, state_c) = net(x, (state_h, state_c))\n",
        "            test_loss = criterion(logits.transpose(1, 2), y)\n",
        "\n",
        "            state_h = state_h.detach()\n",
        "            state_c = state_c.detach()\n",
        "\n",
        "            test_loss_value = test_loss.item()\n",
        "\n",
        "        all_test_losses.append(test_loss_value)\n",
        "\n",
        "\n",
        "        print('')\n",
        "       # print('Current mean of validation loss over batches: {}'.format(mean_all_valid_losses))\n",
        "        print('Current validation loss: {}'.format(valid_loss_value))\n",
        "        print('Validation perplexity: {}'.format(np.exp((valid_loss_value))))\n",
        "        print(' ')\n",
        "        print('Current test loss: {}'.format(test_loss_value))\n",
        "        print('Test perplexity: {}'.format(np.exp((test_loss_value))))\n",
        "\n",
        "        print('')\n",
        "            \n",
        "        plt.figure()                             \n",
        "        plt.xlabel('Epochs'), plt.ylabel('Loss')   \n",
        "        plt.plot(epochnumber, all_train_losses, 'r', epochnumber, all_valid_losses, 'b', epochnumber, all_test_losses, 'g')\n",
        "        plt.legend(['Train Loss','Validation Loss', 'Test Loss'])    \n",
        "   \n",
        "        last= predict(device, net, flags.initial_words_train, n_vocab,\n",
        "                            vocab_to_int, int_to_vocab, flags.predict_top_k)\n",
        "        print('\\nThe last predicted 200 characters are:\\n')\n",
        "        print(last[-200:])\n",
        "\n",
        "    #path = \"/content/drive/My Drive/DL_project/\"\n",
        "    # save the pretrained model if the validation loss is less than minimum validation loss.\n",
        "        if flags.mode == 'pretraining' and e > 0 and valid_loss_value <= min(all_valid_losses) :\n",
        "          if ( datafrom=='Drive'):\n",
        "            torch.save(net.state_dict(), '/content/drive/My Drive/DL_project/net.pth')\n",
        "            print('Saved model')\n",
        "          else:\n",
        "            torch.save(net.state_dict(), '/content/drive/My Drive/net.pth')\n",
        "            print('Saved model')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZE35gpjOaFa",
        "colab_type": "text"
      },
      "source": [
        "##Define a prediction function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6dnXz8s6BSI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(device, net, words, n_vocab, vocab_to_int, int_to_vocab, top_k):\n",
        "    net.eval()\n",
        "\n",
        "    state_h, state_c = net.zero_state(1)\n",
        "    state_h = state_h.to(device)\n",
        "    state_c = state_c.to(device)\n",
        "    for w in words:\n",
        "        ix = torch.tensor([[vocab_to_int[w]]]).to(device)\n",
        "        output, (state_h, state_c) = net(ix, (state_h, state_c))\n",
        "    \n",
        "\n",
        "    _, top_ix = torch.topk(output[0], k=top_k)\n",
        "    choices = top_ix.tolist()\n",
        "    choice = np.random.choice(choices[0]) #A way to avoid always choose like \"and\" \"then\"..... \n",
        "\n",
        "    words.append(int_to_vocab[choice])\n",
        "\n",
        "    for _ in range(100):\n",
        "        ix = torch.tensor([[choice]]).to(device)\n",
        "        output, (state_h, state_c) = net(ix, (state_h, state_c))\n",
        "\n",
        "        _, top_ix = torch.topk(output[0], k=top_k)\n",
        "        choices = top_ix.tolist()\n",
        "        choice = np.random.choice(choices[0])\n",
        "        words.append(int_to_vocab[choice])\n",
        "\n",
        "    print(' '.join(words))\n",
        "    return ' '.join(words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rp4Z_Us8OfT6",
        "colab_type": "text"
      },
      "source": [
        "#***Run model***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKG_14IA6JCF",
        "colab_type": "code",
        "outputId": "d39fec88-74bf-4c27-d87e-985fb94fa29b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------  \n",
            "----------------------Go into training mode--------------------------------  \n",
            "----------------------using friends data------------------------\n",
            "------------------------------------------------------------------------  \n",
            "Vocabulary size 33347\n",
            "Epoch: 0/100 Iteration: 100 \t Loss: 6.115136623382568\n",
            "Epoch: 0/100 Iteration: 200 \t Loss: 5.288741111755371\n",
            "Epoch: 0/100 Iteration: 300 \t Loss: 5.214788913726807\n",
            "Epoch: 0/100 Iteration: 400 \t Loss: 5.281122207641602\n",
            "Epoch: 0/100 Iteration: 500 \t Loss: 4.989944934844971\n",
            "Epoch: 0/100 Iteration: 600 \t Loss: 5.080982208251953\n",
            "Epoch: 0/100 Iteration: 700 \t Loss: 4.910762310028076\n",
            "Final training loss 4.870915412902832\n",
            "\n",
            "Current validation loss: 5.0247297286987305\n",
            "Validation perplexity: 152.1291344167929\n",
            " \n",
            "Current test loss: 4.950284481048584\n",
            "Test perplexity: 141.21513123639608\n",
            "\n",
            "oh and a <unk> of the other . the next is in the same and the door . ] ] the door . ( they the door of a table . ] joey: monica , and monica , and joey and rachel , and joey and phoebe . ) chandler: yeah ! monica: you ? ( he gets the <unk> and rachel to him . ] monica: ( the couch is the couch of her ) , you just know you . chandler: you think ! monica: ( they goes the phone and he starts the couch . ] monica: the door\n",
            "\n",
            "The last predicted 200 characters are:\n",
            "\n",
            "he gets the <unk> and rachel to him . ] monica: ( the couch is the couch of her ) , you just know you . chandler: you think ! monica: ( they goes the phone and he starts the couch . ] monica: the door\n",
            "Epoch: 1/100 Iteration: 100 \t Loss: 4.803447723388672\n",
            "Epoch: 1/100 Iteration: 200 \t Loss: 4.824258327484131\n",
            "Epoch: 1/100 Iteration: 300 \t Loss: 4.760015964508057\n",
            "Epoch: 1/100 Iteration: 400 \t Loss: 4.847779273986816\n",
            "Epoch: 1/100 Iteration: 500 \t Loss: 4.675253391265869\n",
            "Epoch: 1/100 Iteration: 600 \t Loss: 4.657369613647461\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-156-c7bc734e5e35>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-154-0e384d5b26de>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;31m# Reset all gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0;31m# Transfer data to GPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mzero_grad\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    163\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m                     \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m                     \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEGCAYAAAB/+QKOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMzNL5YAwM7NUDggzM0v1/wGtd8ZyVlkcgwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}